{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "from lib._class.environment.ForexEnv import ForexEnv\n",
    "\n",
    "# Agent\n",
    "from lib._class.agent.Agent import Agent\n",
    "\n",
    "# Agent (Matrix)\n",
    "from lib._class.agent.QLearningAgent import QLearningAgent\n",
    "from lib._class.agent.SarsaAgent import SarsaAgent\n",
    "from lib._class.agent.SarsaLambdaAgent import SarsaLambdaAgent\n",
    "\n",
    "# Agent (Neural Network)\n",
    "from lib._class.agent.QNetworkAgent import QNetworkAgent\n",
    "from lib._class.agent.DQNAgent import DQNAgent\n",
    "from lib._class.agent.D2QNAgent import D2QNAgent\n",
    "from lib._class.agent.D3QNAgent import D3QNAgent\n",
    "\n",
    "import lib._util.fileproc as fileproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change tensorflow default behavior (where it uses all of the memory at the outset)\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "# Plotly\n",
    "from plotly.offline import iplot, plot, init_notebook_mode\n",
    "import plotly.graph_objects as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Time measurement\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Sound notification\n",
    "import winsound\n",
    "\n",
    "# Profiling\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH_DATA = 'resources/data/'\n",
    "OUT_PATH_IMAGE   = 'resources/output/image/'\n",
    "OUT_PATH_GRAPH = 'resources/output/graph/'\n",
    "OUT_PATH_FILE = 'resources/output/file/'\n",
    "\n",
    "def plot_graph(data, title, xlabel=None, ylabel=None, generate_file=True):\n",
    "    layout = go.Layout(\n",
    "        title = title,\n",
    "        xaxis = dict(\n",
    "            title=xlabel,\n",
    "            gridcolor='rgb(159, 197, 232)'\n",
    "        ),\n",
    "        yaxis = dict(\n",
    "            title=ylabel,\n",
    "            gridcolor='rgb(159, 197, 232)'\n",
    "        ),\n",
    "        hovermode='x',\n",
    "        showlegend=True,\n",
    "        legend_orientation='h',\n",
    "        plot_bgcolor='rgba(0, 0, 0, 0)'\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.update_yaxes(hoverformat=\".5f\")\n",
    "\n",
    "    if generate_file:\n",
    "        generate_plot(fig, f'{OUT_PATH_GRAPH}', f'{title}.html')\n",
    "    else:\n",
    "        generate_plot(fig)\n",
    "\n",
    "def generate_plot(fig, out_path=None, out_filename=None):\n",
    "    if out_path is None:\n",
    "        iplot(fig)\n",
    "    else:\n",
    "        fileproc.create_directory(out_path)\n",
    "        out_file = f'{out_path}{out_filename}'\n",
    "        plot(fig, filename=out_file, auto_open=False)\n",
    "        \n",
    "        print(f'Generated: {out_file}')\n",
    "\n",
    "def time_taken(seconds):\n",
    "    print(f'\\nTime Taken: {str(timedelta(seconds=seconds))}')\n",
    "    winsound.Beep(frequency=1000, duration=100)\n",
    "    winsound.Beep(frequency=1500, duration=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_pair = 'EURUSD'\n",
    "filename      = f'DAT_ASCII_{currency_pair}_T_201901.csv'\n",
    "\n",
    "env = ForexEnv(SOURCE_PATH_DATA, filename, nrows=30_000, train_size=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "price_types = ['bid', 'ask']\n",
    "for price_index, prices in enumerate([env.bids, env.asks]):\n",
    "    data.append(go.Scattergl(\n",
    "        x = pd.DataFrame(env.datetimes)[0],\n",
    "        y = prices,\n",
    "        mode = 'lines',\n",
    "        name = price_types[price_index].title()\n",
    "    ))\n",
    "\n",
    "title = f'{currency_pair} - Forex Environment ({len(env.datetimes) :,} Timesteps)'\n",
    "plot_graph(data, title, 'Date Time', 'Price')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes, agent_type):\n",
    "    try:\n",
    "        # Training parameters\n",
    "        pretrain_size = 10_000\n",
    "        sample_size = 1_000\n",
    "        memory_size = 1_000_000\n",
    "        neurons=[512]\n",
    "\n",
    "        # Off-Policy agent\n",
    "        if agent_type == 'Normal':\n",
    "            agent = Agent(env)\n",
    "\n",
    "        elif agent_type == 'Q-Learning':\n",
    "            agent = QLearningAgent(env)\n",
    "\n",
    "        elif agent_type == 'Q-Network':\n",
    "            agent = QNetworkAgent(env, pretrain_size=pretrain_size, sample_size=sample_size, memory_size=memory_size)\n",
    "            agent.main_model.model_diagram(OUT_PATH_IMAGE, agent_type)\n",
    "\n",
    "        elif agent_type == 'DQN':\n",
    "            agent = DQNAgent(env, pretrain_size=pretrain_size, sample_size=sample_size, memory_size=memory_size, neurons=neurons)\n",
    "            agent.main_model.model_diagram(OUT_PATH_IMAGE, agent_type)\n",
    "\n",
    "        elif agent_type == 'D2QN':\n",
    "            agent = D2QNAgent(env, pretrain_size=pretrain_size, sample_size=sample_size, memory_size=memory_size, neurons=neurons)\n",
    "            agent.main_model.model_diagram(OUT_PATH_IMAGE, agent_type)\n",
    "            \n",
    "        elif agent_type == 'D3QN':\n",
    "            agent = D3QNAgent(env, pretrain_size=pretrain_size, sample_size=sample_size, memory_size=memory_size, neurons=neurons)\n",
    "            agent.main_model.model_diagram(OUT_PATH_IMAGE, agent_type)\n",
    "\n",
    "        # On-Policy agent\n",
    "        elif agent_type == 'SARSA':\n",
    "            agent = SarsaAgent(env)\n",
    "\n",
    "        elif agent_type == 'SARSA Lambda':\n",
    "            agent = SarsaLambdaAgent(env, episodic_trace=False)\n",
    "\n",
    "\n",
    "        # Performance tracking\n",
    "        result_dict = {\n",
    "            'total_reward': [],\n",
    "            'acct_bal': [],\n",
    "            'trades': [],\n",
    "            'memory_count': []\n",
    "        }\n",
    "\n",
    "        # Training iteration\n",
    "        learn_ep = 0\n",
    "        for episode in range(episodes):\n",
    "            # Performance tracking\n",
    "            total_reward = 0\n",
    "            \n",
    "            # Walkthrough environment\n",
    "            done  = False\n",
    "            state = env.reset()\n",
    "            \n",
    "             # Choose action\n",
    "            scaled_state = env.scale_state(state)\n",
    "            action = agent.choose_action(scaled_state)\n",
    "\n",
    "            while not done: \n",
    "                # Take action\n",
    "                next_state, reward, done, info_dict = env.step(action)\n",
    "                \n",
    "                # Performance tracking\n",
    "                total_reward += reward\n",
    "                \n",
    "                # Choose next action\n",
    "                scaled_next_state = env.scale_state(next_state)\n",
    "                next_action = agent.choose_action(scaled_next_state)\n",
    "\n",
    "                # Learning\n",
    "                scaled_state = env.scale_state(state)\n",
    "                experience = (scaled_state, action, reward, scaled_next_state, done)\n",
    "                learned = agent.learn(experience, next_action, learn_ep)\n",
    "\n",
    "                state  = next_state\n",
    "                action = next_action\n",
    "                \n",
    "            # Increase learned episode\n",
    "            learn_ep += int(learned)\n",
    "\n",
    "            # Result summary\n",
    "            result_dict['total_reward'].append(total_reward)\n",
    "            result_dict['acct_bal'].append(env.trading_params_dict['acct_bal'])\n",
    "            result_dict['trades'].append(env.trading_params_dict['trade_dict'])\n",
    "            result_dict['trades'][-1]['episode'] = episode +1\n",
    "            try:\n",
    "                memory_count = agent.memory.counter\n",
    "            except:\n",
    "                memory_count = -1\n",
    "            result_dict['memory_count'].append(memory_count)\n",
    "\n",
    "            # Progress\n",
    "            # clear_output(wait=True)\n",
    "            ε = agent.hyperparams_dict['epsilon']['value']\n",
    "            α = agent.hyperparams_dict['alpha']['value']\n",
    "            γ = agent.hyperparams_dict['gamma']['value']\n",
    "            \n",
    "            print(f'EP: {episode +1 :,} ({learn_ep :,}) | ε: {ε :.3f} | α: {α :.5f} | γ: {γ :.3f} | SUM(R): {total_reward :,.0f} | M: {memory_count :,}')\n",
    "        return result_dict, agent\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print('\\n!!! KeyboardInterrupt Exception !!!')\n",
    "        return result_dict, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "# agent_type = 'Normal'\n",
    "agent_type = 'Q-Learning'\n",
    "# agent_type = 'SARSA'\n",
    "# agent_type = 'SARSA Lambda'\n",
    "# agent_type = 'Q-Network'\n",
    "# agent_type = 'DQN'\n",
    "# agent_type = 'D2QN'\n",
    "# agent_type = 'D3QN'\n",
    "\n",
    "episodes = 15_000\n",
    "\n",
    "# FOR PROFILING PURPOSE\n",
    "# %lprun -f train \\\n",
    "result_dict, agent = train(episodes, agent_type)\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model_checkpoint(OUT_PATH_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'total_reward': result_dict['total_reward'],\n",
    "    'acct_bal': result_dict['acct_bal'],\n",
    "    'memory_count': result_dict['memory_count']\n",
    "})\n",
    "result_df['episode'] = [x+1 for x in range(len(result_df))]\n",
    "trade_df = pd.concat([pd.DataFrame(x) for x in result_dict['trades']], ignore_index=True)\n",
    "\n",
    "# Profits\n",
    "profit_df = trade_df.groupby('episode').agg({ 'profits': 'sum' }).reset_index()\n",
    "result_df = result_df.merge(profit_df, on='episode', how='left')\n",
    "result_df['profits'] = result_df['profits'].fillna(0)\n",
    "\n",
    "# Used margin\n",
    "open_trade_df = trade_df[trade_df['status'].isin([env.constant_values()['TRADE_STATUS']['OPEN']])]\n",
    "open_trade_df = open_trade_df.groupby('episode').agg(open_count=('price', 'count')).reset_index()\n",
    "\n",
    "result_df = result_df.merge(open_trade_df, on='episode', how='left')\n",
    "result_df['open_count'] = result_df['open_count'].fillna(0)\n",
    "result_df['used_margin'] = result_df['open_count'] * env.trading_params_dict['unit']\n",
    "result_df['equity'] = result_df['acct_bal'] + result_df['used_margin']\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Rolling Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x+1 for x in range(episodes)],\n",
    "    y = result_df.rolling(1_000).mean()['profits'],\n",
    "    mode = 'lines',\n",
    "    name = f'Rolling Profit'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Rolling Profit - {agent_type}'\n",
    "plot_graph(data, title, 'Episode', 'Amount')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Memory Collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x+1 for x in range(episodes)],\n",
    "    y = result_df['memory_count'],\n",
    "    mode = 'lines',\n",
    "    name = f'Memory Collected'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Memory Collected - {agent_type}'\n",
    "plot_graph(data, title, 'Episode', 'Count')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Rolling Step Taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x+1 for x in range(episodes)],\n",
    "    y = result_df.rolling(1_000).mean()['memory_count'].diff(),\n",
    "    mode = 'lines',\n",
    "    name = f'Rolling Step Taken'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Rolling Step Taken - {agent_type}'\n",
    "plot_graph(data, title, 'Episode', 'Count')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Rolling Equity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x+1 for x in range(episodes)],\n",
    "    y = result_df.rolling(1_000).mean()['equity'],\n",
    "    mode = 'lines',\n",
    "    name = 'Rolling Equity'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Rolling Equity - {agent_type}'\n",
    "plot_graph(data, title, 'Episode', 'Equity')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Action Profit-Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "profit_df = trade_df.groupby('episode').agg({\n",
    "    'action': 'first',\n",
    "    'profits': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = ['Buy', 'Sell']\n",
    "for index, action in enumerate([env.constant_values()['TRADE_ACTION']['BUY'], env.constant_values()['TRADE_ACTION']['SELL']]):\n",
    "    pnl_df = profit_df[profit_df['action'] == action]\n",
    "    data.append(go.Scattergl(\n",
    "        x = pnl_df['episode'],\n",
    "        y = pnl_df['profits'],\n",
    "        mode = 'lines',\n",
    "        name = f'{labels[index]} Profit-Loss ({len(pnl_df) :,})'\n",
    "    ))\n",
    "\n",
    "title = f'{currency_pair} - Action Profit-Loss - {agent_type}'\n",
    "plot_graph(data, title, 'Episode', 'Profit-Loss')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# agent.load_model_checkpoint(OUT_PATH_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "price_types = ['bid', 'ask']\n",
    "for price_index, prices in enumerate([env.bids, env.asks]):\n",
    "    data.append(go.Scattergl(\n",
    "        x = pd.DataFrame(env.datetimes)[0],\n",
    "        y = prices,\n",
    "        mode = 'lines',\n",
    "        name = price_types[price_index].title(),\n",
    "        \n",
    "        # Additional settings\n",
    "        hoverinfo='skip'\n",
    "    ))\n",
    "\n",
    "markers = ['triangle-up', 'triangle-down']\n",
    "trade_actions = ['buy', 'sell']\n",
    "for trade_index, trade_action in enumerate(trade_actions):\n",
    "    action_df = trade_df[trade_df['action'] == trade_index]\n",
    "    \n",
    "    data.append(go.Scattergl(\n",
    "        x = action_df['datetime'],\n",
    "        y = action_df['price'],\n",
    "        mode = 'markers',\n",
    "        name = trade_action.title(),\n",
    "        \n",
    "        # Additional settings\n",
    "        marker = dict(\n",
    "            size=15,\n",
    "            symbol=markers[trade_index]\n",
    "        ),\n",
    "        hovertext=[f'Date Time: {row.datetime}<br />Action Index: {row.Index}<br />{\"Open\" if row.Index % 2 == 0 else \"Closed\"} at {row.price}<br />Profit: {row.profits}'\n",
    "                   for row in action_df.itertuples()],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "title = f'{currency_pair} - Trade - {agent_type}'\n",
    "plot_graph(data, title, 'Date Time', 'Price')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Trade Profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "y = trade_df['profits']\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x for x in range(len(trade_df))],\n",
    "    y = y,\n",
    "    mode = 'lines',\n",
    "    name = f'Profits ({sum(y) :,.2f})'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Trade Profits - {agent_type}'\n",
    "plot_graph(data, title, '', 'Amount')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
