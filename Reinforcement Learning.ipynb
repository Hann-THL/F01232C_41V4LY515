{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "from lib._class.environment.ForexEnv import ForexEnv\n",
    "\n",
    "# Agent\n",
    "from lib._class.agent.Agent import Agent\n",
    "\n",
    "# Agent (Matrix)\n",
    "from lib._class.agent.QLearningAgent import QLearningAgent\n",
    "from lib._class.agent.SarsaAgent import SarsaAgent\n",
    "from lib._class.agent.SarsaLambdaAgent import SarsaLambdaAgent\n",
    "\n",
    "# Agent (Neural Network)\n",
    "from lib._class.agent.QNetworkAgent import QNetworkAgent\n",
    "from lib._class.agent.DQNAgent import DQNAgent\n",
    "from lib._class.agent.D2QNAgent import D2QNAgent\n",
    "from lib._class.agent.D3QNAgent import D3QNAgent\n",
    "\n",
    "import lib._util.fileproc as fileproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change tensorflow default behavior (where it uses all of the memory at the outset)\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "# Plotly\n",
    "from plotly.offline import iplot, plot, init_notebook_mode\n",
    "import plotly.graph_objects as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Time measurement\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# Sound notification\n",
    "import winsound\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Profiling\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH_DATA = 'resources/data/'\n",
    "OUT_PATH_IMAGE   = 'resources/output/image/'\n",
    "OUT_PATH_GRAPH = 'resources/output/graph/'\n",
    "OUT_PATH_FILE = 'resources/output/file/'\n",
    "\n",
    "def plot_graph(data, title, xlabel=None, ylabel=None, generate_file=True):\n",
    "    layout = go.Layout(\n",
    "        title = title,\n",
    "        xaxis = dict(\n",
    "            title=xlabel,\n",
    "            gridcolor='rgb(159, 197, 232)'\n",
    "        ),\n",
    "        yaxis = dict(\n",
    "            title=ylabel,\n",
    "            gridcolor='rgb(159, 197, 232)'\n",
    "        ),\n",
    "        hovermode='x',\n",
    "        showlegend=True,\n",
    "        legend_orientation='h',\n",
    "        plot_bgcolor='rgba(0, 0, 0, 0)'\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.update_yaxes(hoverformat=\".5f\")\n",
    "\n",
    "    if generate_file:\n",
    "        generate_plot(fig, f'{OUT_PATH_GRAPH}', f'{title}.html')\n",
    "    else:\n",
    "        generate_plot(fig)\n",
    "\n",
    "def generate_plot(fig, out_path=None, out_filename=None):\n",
    "    if out_path is None:\n",
    "        iplot(fig)\n",
    "    else:\n",
    "        fileproc.create_directory(out_path)\n",
    "        out_file = f'{out_path}{out_filename}'\n",
    "        plot(fig, filename=out_file, auto_open=False)\n",
    "        \n",
    "        print(f'Generated: {out_file}')\n",
    "\n",
    "def time_taken(seconds):\n",
    "    print(f'\\nTime Taken: {str(timedelta(seconds=seconds))}')\n",
    "    winsound.Beep(frequency=1000, duration=100)\n",
    "    winsound.Beep(frequency=1500, duration=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_pair = 'EURUSD'\n",
    "filename      = f'DAT_ASCII_{currency_pair}_T_201901-201906.csv'\n",
    "\n",
    "env = ForexEnv(SOURCE_PATH_DATA, filename, train_size=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "price_types = ['bid', 'ask', 'bid RSI', 'ask RSI']\n",
    "for price_index, prices in enumerate([env.bids, env.asks, env.bids_rsi, env.asks_rsi]):\n",
    "    data.append(go.Scattergl(\n",
    "        x = pd.DataFrame(env.datetimes)[0],\n",
    "        y = prices,\n",
    "        mode = 'lines',\n",
    "        name = price_types[price_index].title()\n",
    "    ))\n",
    "\n",
    "title = f'{currency_pair} - Forex Environment ({len(env.datetimes) :,} Timesteps)'\n",
    "plot_graph(data, title, 'Date Time', 'Price')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes, agent_type):\n",
    "    try:\n",
    "        # Training parameters\n",
    "        pretrain_size = 10_000\n",
    "        sample_size   = 1_000\n",
    "        memory_size   = 50_000\n",
    "        neurons       = [16, 32, 64]\n",
    "\n",
    "        # Agent\n",
    "        if agent_type == 'Normal':\n",
    "            agent = Agent(env)\n",
    "\n",
    "        # Off-Policy agent\n",
    "        elif agent_type == 'Q-Learning':\n",
    "            agent = QLearningAgent(env)\n",
    "\n",
    "        elif agent_type == 'Q-Network':\n",
    "            agent = QNetworkAgent(env, pretrain_size=pretrain_size, sample_size=sample_size, memory_size=memory_size)\n",
    "            agent.main_model.model_diagram(OUT_PATH_IMAGE, agent_type)\n",
    "\n",
    "        elif agent_type == 'DQN':\n",
    "            agent = DQNAgent(env, pretrain_size=pretrain_size, sample_size=sample_size, memory_size=memory_size, neurons=neurons)\n",
    "            agent.main_model.model_diagram(OUT_PATH_IMAGE, agent_type)\n",
    "\n",
    "        elif agent_type == 'D2QN':\n",
    "            agent = D2QNAgent(env, pretrain_size=pretrain_size, sample_size=sample_size, memory_size=memory_size, neurons=neurons)\n",
    "            agent.main_model.model_diagram(OUT_PATH_IMAGE, agent_type)\n",
    "            \n",
    "        elif agent_type == 'D3QN':\n",
    "            agent = D3QNAgent(env, pretrain_size=pretrain_size, sample_size=sample_size, memory_size=memory_size, neurons=neurons)\n",
    "            agent.main_model.model_diagram(OUT_PATH_IMAGE, agent_type)\n",
    "\n",
    "        # On-Policy agent\n",
    "        elif agent_type == 'SARSA':\n",
    "            agent = SarsaAgent(env)\n",
    "\n",
    "        elif agent_type == 'SARSA Lambda':\n",
    "            agent = SarsaLambdaAgent(env, episodic_trace=False)\n",
    "\n",
    "\n",
    "        # Performance tracking\n",
    "        result_dict = {\n",
    "            'total_reward': [],\n",
    "            'trades': [],\n",
    "            'step_count': [],\n",
    "            'memory_count': []\n",
    "        }\n",
    "\n",
    "        # Training iteration\n",
    "        learn_ep = 0\n",
    "        for episode in range(episodes):\n",
    "            # Performance tracking\n",
    "            total_timestep = 0\n",
    "            total_reward   = 0\n",
    "            min_reward     = 0\n",
    "            max_reward     = 0\n",
    "            \n",
    "            # Walkthrough environment\n",
    "            done  = False\n",
    "            state = env.reset()\n",
    "            \n",
    "             # Choose action\n",
    "            norm_state = env.normalize_state(state)\n",
    "            action     = agent.choose_action(norm_state)\n",
    "\n",
    "            while not done: \n",
    "                # Take action\n",
    "                next_state, reward, done, info_dict = env.step(action)\n",
    "                \n",
    "                # Performance tracking\n",
    "                total_reward   += reward\n",
    "                total_timestep += 1\n",
    "                min_reward     = reward if reward < min_reward else min_reward\n",
    "                max_reward     = reward if reward > max_reward else max_reward\n",
    "                \n",
    "                # Reward engineering\n",
    "                if info_dict['have_open']:\n",
    "                    entry_action = info_dict['entry_action']\n",
    "                    \n",
    "                    # Encourage agent to sell (close trade) near 70% RSI\n",
    "                    if entry_action == env.constant_values()['TRADE_ACTION']['BUY']:\n",
    "                        distance_factor = 1 - round((70 - info_dict['bid_rsi']) / 70, 2)\n",
    "                        reward = reward * distance_factor\n",
    "                    \n",
    "                    # Encourage agent to buy (close trade) near 30% RSI\n",
    "                    elif entry_action == env.constant_values()['TRADE_ACTION']['SELL']:\n",
    "                        distance_factor = 1 - round((info_dict['ask_rsi'] - 30) / 30, 2)\n",
    "                        reward = reward * distance_factor\n",
    "                \n",
    "                # Positive reward function\n",
    "                reward = reward if reward > 0 else 0\n",
    "                \n",
    "                # Normalize reward\n",
    "                reward = env.normalize_reward(reward)\n",
    "                \n",
    "                # Choose next action\n",
    "                norm_next_state = env.normalize_state(next_state)\n",
    "                next_action     = agent.choose_action(norm_next_state)\n",
    "\n",
    "                # Learning\n",
    "                norm_state = env.normalize_state(state)\n",
    "                experience = (norm_state, action, reward, norm_next_state, done)\n",
    "                learned    = agent.learn(experience, next_action, learn_ep)\n",
    "\n",
    "                state  = next_state\n",
    "                action = next_action\n",
    "                \n",
    "            # Increase learned episode\n",
    "            learn_ep += int(learned)\n",
    "\n",
    "            # Result summary\n",
    "            result_dict['trades'].append(env.trade_dict)\n",
    "            result_dict['trades'][-1]['episode'] = episode +1\n",
    "            result_dict['step_count'].append(total_timestep)\n",
    "            try:\n",
    "                memory_count = agent.memory.counter\n",
    "            except:\n",
    "                memory_count = -1\n",
    "            result_dict['memory_count'].append(memory_count)\n",
    "\n",
    "            # Progress\n",
    "            # clear_output(wait=True)\n",
    "            ε = agent.hyperparams_dict['epsilon']['value']\n",
    "            \n",
    "            print(f'EP: {episode +1 :,} ({learn_ep :,}) | ε: {ε :.3f} | SUM(R): {total_reward :,.2f} | MAX(R): {max_reward :,.2f} | MIN(R): {min_reward :,.2f} | SUM(T): {total_timestep} | M: {memory_count :,}')\n",
    "        return result_dict, agent\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print('\\n!!! KeyboardInterrupt Exception !!!')\n",
    "        return result_dict, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "# agent_type = 'Normal'\n",
    "# agent_type = 'Q-Learning'\n",
    "# agent_type = 'SARSA'\n",
    "# agent_type = 'SARSA Lambda'\n",
    "# agent_type = 'Q-Network'\n",
    "agent_type = 'DQN'\n",
    "# agent_type = 'D2QN'\n",
    "# agent_type = 'D3QN'\n",
    "\n",
    "episodes = 20_000\n",
    "\n",
    "# FOR PROFILING PURPOSE\n",
    "# %lprun -f train \\\n",
    "result_dict, agent = train(episodes, agent_type)\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model_checkpoint(OUT_PATH_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'memory_count': result_dict['memory_count'],\n",
    "    'step_count': result_dict['step_count']\n",
    "})\n",
    "result_df['episode'] = [x+1 for x in range(len(result_df))]\n",
    "trade_df = pd.concat([pd.DataFrame(x) for x in result_dict['trades']], ignore_index=True)\n",
    "\n",
    "# Profits\n",
    "profit_df = trade_df.groupby('episode').agg({ 'profits': 'sum' }).reset_index()\n",
    "result_df = result_df.merge(profit_df, on='episode', how='left')\n",
    "result_df['profits'] = result_df['profits'].fillna(0)\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Rolling Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x+1 for x in range(episodes)],\n",
    "    y = result_df.rolling(1_000).mean()['profits'],\n",
    "    mode = 'lines',\n",
    "    name = f'Rolling Profit'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Rolling Profit - {agent_type}'\n",
    "plot_graph(data, title, 'Episode', 'Amount')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Memory Collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x+1 for x in range(episodes)],\n",
    "    y = result_df['memory_count'],\n",
    "    mode = 'lines',\n",
    "    name = f'Memory Collected'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Memory Collected - {agent_type}'\n",
    "plot_graph(data, title, 'Episode', 'Count')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Rolling Step Taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x+1 for x in range(episodes)],\n",
    "    y = result_df.rolling(1_000).mean()['step_count'],\n",
    "    mode = 'lines',\n",
    "    name = f'Rolling Step Taken'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Rolling Step Taken - {agent_type}'\n",
    "plot_graph(data, title, 'Episode', 'Count')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Action Profit-Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "closed_df = trade_df[trade_df['status'].isin([\n",
    "    env.constant_values()['TRADE_STATUS']['CLOSE_TRADE'],\n",
    "    env.constant_values()['TRADE_STATUS']['MARGIN_CALL']]\n",
    ")]\n",
    "profit_df = closed_df.groupby(['episode', 'action', 'status']).agg({\n",
    "    'profits': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = ['Sell Entry', 'Buy Entry']\n",
    "for index, action in enumerate([env.constant_values()['TRADE_ACTION']['BUY'], env.constant_values()['TRADE_ACTION']['SELL']]):\n",
    "    for status in [env.constant_values()['TRADE_STATUS']['CLOSE_TRADE'], env.constant_values()['TRADE_STATUS']['MARGIN_CALL']]:\n",
    "        pnl_df = profit_df[(profit_df['action'] == action) & (profit_df['status'] == status)]\n",
    "        data.append(go.Scattergl(\n",
    "            x = pnl_df['episode'],\n",
    "            y = pnl_df['profits'],\n",
    "            mode = 'lines',\n",
    "            name = f'{labels[index]} - {status} ({len(pnl_df) :,})'\n",
    "        ))\n",
    "\n",
    "title = f'{currency_pair} - Action Profit-Loss - {agent_type}'\n",
    "plot_graph(data, title, 'Episode', 'Profit-Loss')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# agent.load_model_checkpoint(OUT_PATH_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "price_types = ['bid', 'ask']\n",
    "for price_index, prices in enumerate([env.bids, env.asks]):\n",
    "    data.append(go.Scattergl(\n",
    "        x = pd.DataFrame(env.datetimes)[0],\n",
    "        y = prices,\n",
    "        mode = 'lines',\n",
    "        name = price_types[price_index].title(),\n",
    "        \n",
    "        # Additional settings\n",
    "        hoverinfo='skip'\n",
    "    ))\n",
    "\n",
    "markers = ['triangle-up', 'triangle-down']\n",
    "trade_actions = ['buy', 'sell']\n",
    "for trade_index, trade_action in enumerate(trade_actions):\n",
    "    action_df = trade_df[trade_df['action'] == trade_index]\n",
    "    \n",
    "    data.append(go.Scattergl(\n",
    "        x = action_df['datetime'],\n",
    "        y = action_df['price'],\n",
    "        mode = 'markers',\n",
    "        name = trade_action.title(),\n",
    "        \n",
    "        # Additional settings\n",
    "        marker = dict(\n",
    "            size=15,\n",
    "            symbol=markers[trade_index]\n",
    "        ),\n",
    "        hovertext=[f'Date Time: {row.datetime}<br />Action Index: {row.Index}<br />{\"Open\" if row.Index % 2 == 0 else \"Closed\"} at {row.price}<br />Profit: {row.profits}'\n",
    "                   for row in action_df.itertuples()],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "title = f'{currency_pair} - Trade - {agent_type}'\n",
    "plot_graph(data, title, 'Date Time', 'Price')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Trade Profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "y = trade_df['profits']\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x for x in range(len(trade_df))],\n",
    "    y = y,\n",
    "    mode = 'lines',\n",
    "    name = f'Profits ({sum(y) :,.2f})'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Trade Profits - {agent_type}'\n",
    "plot_graph(data, title, '', 'Amount')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
