{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib._util.visualplot as vp\n",
    "import lib._util.fileproc as fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "from lib._class.environment.ForexEnv import ForexEnv\n",
    "\n",
    "# Agent\n",
    "from lib._class.agent.Agent import Agent\n",
    "\n",
    "# Agent (Matrix)\n",
    "from lib._class.agent.QLearningAgent import QLearningAgent\n",
    "from lib._class.agent.SarsaAgent import SarsaAgent\n",
    "from lib._class.agent.SarsaLambdaAgent import SarsaLambdaAgent\n",
    "\n",
    "# Agent (Neural Network)\n",
    "from lib._class.agent.QNetworkAgent import QNetworkAgent\n",
    "from lib._class.agent.DQNAgent import DQNAgent\n",
    "from lib._class.agent.D2QNAgent import D2QNAgent\n",
    "from lib._class.agent.D3QNAgent import D3QNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change tensorflow default behavior (where it uses all of the memory at the outset)\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "# Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Time measurement\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# Sound notification\n",
    "import winsound\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Profiling\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH_DATA = 'resources/output/eda/file/'\n",
    "OUT_PATH_IMAGE   = 'resources/output/RL/image/'\n",
    "OUT_PATH_GRAPH = 'resources/output/RL/graph/'\n",
    "OUT_PATH_FILE = 'resources/output/RL/file/'\n",
    "\n",
    "def time_taken(seconds):\n",
    "    print(f'\\nTime Taken: {str(timedelta(seconds=seconds))}')\n",
    "    winsound.Beep(frequency=1000, duration=100)\n",
    "    winsound.Beep(frequency=1500, duration=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_pair = 'EURUSD'\n",
    "filename      = f'DAT_ASCII_{currency_pair}_Day_Feature.csv'\n",
    "# filename      = f'DAT_ASCII_{currency_pair}_Hour_Feature.csv'\n",
    "# filename      = f'DAT_ASCII_{currency_pair}_Min_Feature.csv'\n",
    "\n",
    "env = ForexEnv(SOURCE_PATH_DATA, filename, train_size=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "price_types = ['bid', 'ask']\n",
    "for price_index, prices in enumerate([env.bids, env.asks]):\n",
    "    data.append(go.Scattergl(\n",
    "        x = pd.DataFrame(env.datetimes)[0],\n",
    "        y = prices,\n",
    "        mode = 'lines',\n",
    "        name = price_types[price_index].title()\n",
    "    ))\n",
    "\n",
    "title = f'{currency_pair} - Forex Environment ({len(env.datetimes) :,} Timesteps)'\n",
    "vp.plot_graph(data, title, 'Date Time', 'Price', out_path=OUT_PATH_GRAPH)\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes, agent_type):\n",
    "    try:\n",
    "        # Training parameters\n",
    "        pretrain_size = 2_500\n",
    "        sample_size   = 1_000\n",
    "        memory_size   = 10_000\n",
    "        neurons       = [32, 64, 128]\n",
    "\n",
    "        # Agent\n",
    "        if agent_type == 'Normal':\n",
    "            agent = Agent(env)\n",
    "\n",
    "        # Off-Policy agent\n",
    "        elif agent_type == 'Q-Learning':\n",
    "            agent = QLearningAgent(env)\n",
    "\n",
    "        elif agent_type == 'Q-Network':\n",
    "            agent = QNetworkAgent(env, pretrain_size=pretrain_size, sample_size=sample_size, memory_size=memory_size)\n",
    "            agent.main_model.model_diagram(OUT_PATH_IMAGE, agent_type)\n",
    "\n",
    "        elif agent_type == 'DQN':\n",
    "            agent = DQNAgent(env, pretrain_size=pretrain_size, sample_size=sample_size, memory_size=memory_size, neurons=neurons)\n",
    "            agent.main_model.model_diagram(OUT_PATH_IMAGE, agent_type)\n",
    "\n",
    "        elif agent_type == 'D2QN':\n",
    "            agent = D2QNAgent(env, pretrain_size=pretrain_size, sample_size=sample_size, memory_size=memory_size, neurons=neurons)\n",
    "            agent.main_model.model_diagram(OUT_PATH_IMAGE, agent_type)\n",
    "            \n",
    "        elif agent_type == 'D3QN':\n",
    "            agent = D3QNAgent(env, pretrain_size=pretrain_size, sample_size=sample_size, memory_size=memory_size, neurons=neurons)\n",
    "            agent.main_model.model_diagram(OUT_PATH_IMAGE, agent_type)\n",
    "\n",
    "        # On-Policy agent\n",
    "        elif agent_type == 'SARSA':\n",
    "            agent = SarsaAgent(env)\n",
    "\n",
    "        elif agent_type == 'SARSA Lambda':\n",
    "            agent = SarsaLambdaAgent(env, episodic_trace=False)\n",
    "\n",
    "\n",
    "        # Performance tracking\n",
    "        result_dict = {\n",
    "            'total_reward': [],\n",
    "            'max_reward':   [],\n",
    "            'min_reward':   [],\n",
    "            'trades':       [],\n",
    "            'step_count':   [],\n",
    "            'memory_count': []\n",
    "        }\n",
    "\n",
    "        # Training iteration\n",
    "        learn_ep = 0\n",
    "        for episode in range(episodes):\n",
    "            # Performance tracking\n",
    "            total_timestep = 0\n",
    "            total_reward   = 0\n",
    "            min_reward     = 0\n",
    "            max_reward     = 0\n",
    "            \n",
    "            # Reward function\n",
    "            avg_rewards = []\n",
    "            \n",
    "            # Walkthrough environment\n",
    "            done  = False\n",
    "            state = env.reset()\n",
    "            \n",
    "             # Choose action\n",
    "            norm_state = env.normalize_state(state)\n",
    "            action     = agent.choose_action(norm_state)\n",
    "\n",
    "            while not done:\n",
    "                # Take action\n",
    "                next_state, reward, done, info_dict = env.step(action)\n",
    "                trade_done       = info_dict['trade_done']\n",
    "                trade_next_state = info_dict['trade_next_state']\n",
    "                \n",
    "                # Performance tracking\n",
    "                total_reward   += reward\n",
    "                total_timestep += 1\n",
    "                min_reward     = reward if reward < min_reward else min_reward\n",
    "                max_reward     = reward if reward > max_reward else max_reward\n",
    "                \n",
    "                # Reward engineering\n",
    "                entry_action = next_state[1]\n",
    "                reward       = info_dict['roi'] if trade_done else info_dict['float_roi'] if info_dict['have_open'] else 0\n",
    "                \n",
    "                if entry_action != -1:\n",
    "                    avg_rewards.append(reward)\n",
    "                    reward = np.mean(avg_rewards)\n",
    "                    reward = round(reward, 5)\n",
    "                \n",
    "                if trade_done:\n",
    "                    avg_rewards = []\n",
    "                \n",
    "                # Normalize reward\n",
    "                reward = env.normalize_reward(reward)\n",
    "                \n",
    "                # Choose next action (based on trade state)\n",
    "                norm_next_state = env.normalize_state(trade_next_state)\n",
    "                next_action     = agent.choose_action(norm_next_state)\n",
    "\n",
    "                # Learning\n",
    "                norm_state = env.normalize_state(state)\n",
    "                experience = (norm_state, action, reward, norm_next_state, trade_done)\n",
    "                learned    = agent.learn(experience, next_action, learn_ep)\n",
    "                \n",
    "                # Choose next action (based on actual state)\n",
    "                norm_next_state = env.normalize_state(next_state)\n",
    "                next_action     = agent.choose_action(norm_next_state)\n",
    "\n",
    "                state  = next_state\n",
    "                action = next_action\n",
    "                \n",
    "            # Increase learned episode\n",
    "            learn_ep += int(learned)\n",
    "\n",
    "            # Result summary\n",
    "            result_dict['total_reward'].append(total_reward)\n",
    "            result_dict['max_reward'].append(max_reward)\n",
    "            result_dict['min_reward'].append(min_reward)\n",
    "            result_dict['trades'].append(env.trade_dict)\n",
    "            result_dict['trades'][-1]['episode'] = episode +1\n",
    "            result_dict['step_count'].append(total_timestep)\n",
    "            try:\n",
    "                memory_count = agent.memory.counter\n",
    "            except:\n",
    "                memory_count = -1\n",
    "            result_dict['memory_count'].append(memory_count)\n",
    "\n",
    "            # Progress\n",
    "            # clear_output(wait=True)\n",
    "            ε = agent.hyperparams_dict['epsilon']['value']\n",
    "            \n",
    "            print(f'EP: {episode +1 :,} ({learn_ep :,}) | ε: {ε :.3f} | SUM(R): {total_reward :>7,.1f} | MAX(R): {max_reward :>7,.1f} | MIN(R): {min_reward :>7,.1f} | SUM(T): {total_timestep :>5} | M: {memory_count :,}')\n",
    "        return result_dict, agent\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print('\\n!!! KeyboardInterrupt Exception !!!')\n",
    "        return result_dict, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "# agent_type = 'Normal'\n",
    "# agent_type = 'Q-Learning'\n",
    "# agent_type = 'SARSA'\n",
    "# agent_type = 'SARSA Lambda'\n",
    "# agent_type = 'Q-Network'\n",
    "agent_type = 'DQN'\n",
    "# agent_type = 'D2QN'\n",
    "# agent_type = 'D3QN'\n",
    "\n",
    "episodes = 20_000\n",
    "\n",
    "# FOR PROFILING PURPOSE\n",
    "# %lprun -f train \\\n",
    "result_dict, agent = train(episodes, agent_type)\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model_checkpoint(OUT_PATH_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'total_reward': result_dict['total_reward'],\n",
    "    'max_reward':   result_dict['max_reward'],\n",
    "    'min_reward':   result_dict['min_reward'],\n",
    "    'memory_count': result_dict['memory_count'],\n",
    "    'step_count':   result_dict['step_count']\n",
    "})\n",
    "result_df['episode'] = [x+1 for x in range(len(result_df))]\n",
    "trade_df = pd.concat([pd.DataFrame(x) for x in result_dict['trades']], ignore_index=True)\n",
    "\n",
    "# Profits\n",
    "profit_df = trade_df.groupby('episode').agg({\n",
    "    'profits': 'sum',\n",
    "    'pip_change': 'sum'\n",
    "}).reset_index()\n",
    "result_df = result_df.merge(profit_df, on='episode', how='left')\n",
    "result_df['profits'] = result_df['profits'].fillna(0)\n",
    "result_df['pip_change'] = result_df['pip_change'].fillna(0)\n",
    "\n",
    "# Export trades\n",
    "fp.generate_csv(trade_df, out_path=OUT_PATH_FILE, out_filename=f'{agent_type}_trades.csv', export_index=False)\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Rolling Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "for reward_type in ['total_reward', 'max_reward', 'min_reward']:\n",
    "    data.append(go.Scattergl(\n",
    "        x = [x+1 for x in range(episodes)],\n",
    "        y = result_df.rolling(1_000).mean()[reward_type],\n",
    "        mode = 'lines',\n",
    "        name = f'Rolling {reward_type.title()}'\n",
    "    ))\n",
    "\n",
    "title = f'{currency_pair} - Rolling Reward - {agent_type}'\n",
    "vp.plot_graph(data, title, 'Episode', 'Value', out_path=OUT_PATH_GRAPH)\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Rolling Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "for earn_type in ['profits', 'pip_change']:\n",
    "    data.append(go.Scattergl(\n",
    "        x = [x+1 for x in range(episodes)],\n",
    "        y = result_df.rolling(1_000).mean()[earn_type],\n",
    "        mode = 'lines',\n",
    "        name = f'Rolling {earn_type.title()}'\n",
    "    ))\n",
    "\n",
    "title = f'{currency_pair} - Rolling Profit - {agent_type}'\n",
    "vp.plot_graph(data, title, 'Episode', 'Amount', out_path=OUT_PATH_GRAPH)\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Memory Collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x+1 for x in range(episodes)],\n",
    "    y = result_df['memory_count'],\n",
    "    mode = 'lines',\n",
    "    name = f'Memory Collected'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Memory Collected - {agent_type}'\n",
    "vp.plot_graph(data, title, 'Episode', 'Count', out_path=OUT_PATH_GRAPH)\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Rolling Step Taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x+1 for x in range(episodes)],\n",
    "    y = result_df.rolling(1_000).mean()['step_count'],\n",
    "    mode = 'lines',\n",
    "    name = f'Rolling Step Taken'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Rolling Step Taken - {agent_type}'\n",
    "vp.plot_graph(data, title, 'Episode', 'Count', out_path=OUT_PATH_GRAPH)\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Action Profit-Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "closed_df = trade_df[trade_df['status'].isin([\n",
    "    env.constant_values()['TRADE_STATUS']['CLOSE_TRADE'],\n",
    "    env.constant_values()['TRADE_STATUS']['MARGIN_CALL']]\n",
    ")]\n",
    "profit_df = closed_df.groupby(['episode', 'action', 'status']).agg({\n",
    "    'profits': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = ['Sell Entry', 'Buy Entry']\n",
    "for index, action in enumerate([env.constant_values()['TRADE_ACTION']['BUY'], env.constant_values()['TRADE_ACTION']['SELL']]):\n",
    "    for status in [env.constant_values()['TRADE_STATUS']['CLOSE_TRADE'], env.constant_values()['TRADE_STATUS']['MARGIN_CALL']]:\n",
    "        pnl_df = profit_df[(profit_df['action'] == action) & (profit_df['status'] == status)]\n",
    "        data.append(go.Scattergl(\n",
    "            x = pnl_df['episode'],\n",
    "            y = pnl_df['profits'],\n",
    "            mode = 'lines',\n",
    "            name = f'{labels[index]} - {status} ({len(pnl_df) :,})'\n",
    "        ))\n",
    "\n",
    "title = f'{currency_pair} - Action Profit-Loss - {agent_type}'\n",
    "vp.plot_graph(data, title, 'Episode', 'Profit-Loss', out_path=OUT_PATH_GRAPH)\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# agent.load_model_checkpoint(OUT_PATH_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "price_types = ['bid', 'ask']\n",
    "for price_index, prices in enumerate([env.bids, env.asks]):\n",
    "    data.append(go.Scattergl(\n",
    "        x = pd.DataFrame(env.datetimes)[0],\n",
    "        y = prices,\n",
    "        mode = 'lines',\n",
    "        name = price_types[price_index].title(),\n",
    "        \n",
    "        # Additional settings\n",
    "        hoverinfo='skip'\n",
    "    ))\n",
    "\n",
    "markers = ['triangle-up', 'triangle-down']\n",
    "trade_actions = ['buy', 'sell']\n",
    "for trade_index, trade_action in enumerate(trade_actions):\n",
    "    action_df = trade_df[trade_df['action'] == trade_index]\n",
    "    \n",
    "    data.append(go.Scattergl(\n",
    "        x = action_df['datetime'],\n",
    "        y = action_df['price'],\n",
    "        mode = 'markers',\n",
    "        name = trade_action.title(),\n",
    "        \n",
    "        # Additional settings\n",
    "        marker = dict(\n",
    "            size=15,\n",
    "            symbol=markers[trade_index]\n",
    "        ),\n",
    "        hovertext=[f'Date Time: {row.datetime}<br />Action Index: {row.Index}<br />{\"Open\" if row.Index % 2 == 0 else \"Closed\"} at {row.price}<br />Profit: {row.profits}'\n",
    "                   for row in action_df.itertuples()],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "title = f'{currency_pair} - Trade - {agent_type}'\n",
    "vp.plot_graph(data, title, 'Date Time', 'Price', out_path=OUT_PATH_GRAPH)\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Trade Profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "y = trade_df['profits']\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x for x in range(len(trade_df))],\n",
    "    y = y,\n",
    "    mode = 'lines',\n",
    "    name = f'Profits ({sum(y) :,.2f})'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Trade Profits - {agent_type}'\n",
    "vp.plot_graph(data, title, '', 'Amount', out_path=OUT_PATH_GRAPH)\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
