{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Add, Subtract, Lambda\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "# Change tensorflow default behavior (where it uses all of the memory at the outset)\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "# Raise warning as error to capture floating-point errors during computation:\n",
    "# https://stackoverflow.com/questions/34955158/what-might-be-the-cause-of-invalid-value-encountered-in-less-equal-in-numpy/34955622\n",
    "# https://www.soa.org/news-and-publications/newsletters/compact/2014/may/com-2014-iss51/losing-my-precision-tips-for-handling-tricky-floating-point-arithmetic/\n",
    "import numpy as np\n",
    "# np.seterr(all='raise')\n",
    "\n",
    "# Plotly\n",
    "from plotly.offline import iplot, plot, init_notebook_mode\n",
    "import plotly.graph_objects as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Time measurement\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Sound notification\n",
    "import winsound\n",
    "\n",
    "# Profiling\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH_DATA = 'resources/data/'\n",
    "OUT_PATH_IMAGE   = 'resources/output/image/'\n",
    "OUT_PATH_GRAPH = 'resources/output/graph/'\n",
    "OUT_PATH_FILE = 'resources/output/file/'\n",
    "\n",
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def plot_graph(data, title, xlabel=None, ylabel=None, generate_file=True):\n",
    "    layout = go.Layout(\n",
    "        title = title,\n",
    "        xaxis = dict(\n",
    "            title=xlabel,\n",
    "            gridcolor='rgb(159, 197, 232)'\n",
    "        ),\n",
    "        yaxis = dict(\n",
    "            title=ylabel,\n",
    "            gridcolor='rgb(159, 197, 232)'\n",
    "        ),\n",
    "        hovermode='x',\n",
    "        showlegend=True,\n",
    "        legend_orientation='h',\n",
    "        plot_bgcolor='rgba(0, 0, 0, 0)'\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.update_yaxes(hoverformat=\".5f\")\n",
    "\n",
    "    if generate_file:\n",
    "        generate_plot(fig, f'{OUT_PATH_GRAPH}', f'{title}.html')\n",
    "    else:\n",
    "        generate_plot(fig)\n",
    "\n",
    "def generate_plot(fig, out_path=None, out_filename=None):\n",
    "    if out_path is None:\n",
    "        iplot(fig)\n",
    "    else:\n",
    "        create_directory(out_path)\n",
    "        out_file = f'{out_path}{out_filename}'\n",
    "        plot(fig, filename=out_file, auto_open=False)\n",
    "        \n",
    "        print(f'Generated: {out_file}')\n",
    "\n",
    "def generate_csv(df, out_path, out_filename, export_index=None):\n",
    "    create_directory(out_path)\n",
    "    out_file = f'{out_path}{out_filename}'\n",
    "    df.to_csv(out_file, sep=';', index=export_index, header=True)\n",
    "    \n",
    "    print(f'Generated: {out_file}')\n",
    "\n",
    "def time_taken(seconds):\n",
    "    print(f'\\nTime Taken: {str(timedelta(seconds=seconds))}')\n",
    "    winsound.Beep(frequency=1000, duration=100)\n",
    "    winsound.Beep(frequency=1500, duration=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(x, min_value, max_value, precision=5):\n",
    "    # Reference: https://www.codecademy.com/articles/normalization\n",
    "    return np.round((x - min_value) / (max_value - min_value), precision)\n",
    "\n",
    "def zero_one_scale(x, precision=5):\n",
    "    # Reference: https://stackoverflow.com/questions/42140347/normalize-any-value-in-range-inf-inf-to-0-1-is-it-possible\n",
    "    return np.round((1 + x / (1 + abs(x))) * .5, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForexEnv:\n",
    "    def __init__(self, source_path, filename, nrows=None, train_size=.7, train=True, random_size=.8):\n",
    "        source_path = SOURCE_PATH_DATA\n",
    "        filename    = f'DAT_ASCII_{currency_pair}_T_201901.csv'\n",
    "        self.__train_test_split(source_path, filename, nrows=nrows, train_size=train_size, train=train)\n",
    "        \n",
    "        self.random_range = int(len(self.indexes) * random_size)\n",
    "        \n",
    "    def __train_test_split(self, source_path, filename, chunk_size=50_000, nrows=None, train_size=.7, train=True):\n",
    "        source_file = f'{source_path}{filename}'\n",
    "        df_chunks = pd.read_csv(source_file, sep=',',\n",
    "                                header=None, names=['datetime', 'bid', 'ask', 'vol'],\n",
    "                                usecols=['datetime', 'bid', 'ask'],\n",
    "                                parse_dates=['datetime'],\n",
    "                                date_parser=lambda x: pd.to_datetime(x, format=\"%Y%m%d %H%M%S%f\"),\n",
    "                                chunksize=chunk_size, nrows=nrows)\n",
    "        timeseries_df = pd.concat(df_chunks)\n",
    "        \n",
    "        row_count  = len(timeseries_df) if nrows is None else nrows\n",
    "        split_size = round(row_count * train_size)\n",
    "        \n",
    "        if train:\n",
    "            timeseries_df = timeseries_df[:split_size].reset_index().drop(columns=['index'])\n",
    "        else:\n",
    "            timeseries_df = timeseries_df[split_size:].reset_index().drop(columns=['index'])\n",
    "        \n",
    "        self.indexes   = timeseries_df.index.values\n",
    "        self.datetimes = timeseries_df['datetime'].values\n",
    "        self.bids      = timeseries_df['bid'].values\n",
    "        self.asks      = timeseries_df['ask'].values\n",
    "        \n",
    "    def constant_values(self):\n",
    "        return {\n",
    "            'TRADE_STATUS': {\n",
    "                'OPEN': 'OPEN',\n",
    "                'CLOSE': 'CLOSED',\n",
    "                'CLOSE_TRADE': 'CLOSE_TRADE'\n",
    "            },\n",
    "            'TRADE_ACTION': {\n",
    "                'DEFAULT': -1,\n",
    "                'BUY': 0,\n",
    "                'SELL': 1,\n",
    "                'HOLD': 2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def state_space(self):\n",
    "        return np.array(['entry_action', 'bid_fluctuation_pct', 'ask_fluctuation_pct', 'equity_pct'])\n",
    "        \n",
    "    def state_size(self):\n",
    "        return len(self.state_space())\n",
    "        \n",
    "    def action_space(self):\n",
    "        const_action_dict = self.constant_values()['TRADE_ACTION']\n",
    "        return [const_action_dict['BUY'], const_action_dict['SELL'], const_action_dict['HOLD']]\n",
    "        \n",
    "    def action_size(self):\n",
    "        return len(self.action_space())\n",
    "        \n",
    "    def available_actions(self):\n",
    "        const_status_dict = self.constant_values()['TRADE_STATUS']\n",
    "        actions = self.action_space()\n",
    "        \n",
    "        # Have open trades\n",
    "        trade_dict = self.trading_params_dict['trade_dict']\n",
    "        if const_status_dict['OPEN'] in trade_dict['status']:\n",
    "            open_index  = trade_dict['status'].index(const_status_dict['OPEN'])\n",
    "            open_action = trade_dict['action'][open_index]\n",
    "\n",
    "            # Ensure agent is able to have only 1 open trade while trading\n",
    "            actions.remove(open_action)\n",
    "        return actions\n",
    "    \n",
    "    def __price_by_action(self, action, bid, ask, closed_trade):\n",
    "        const_action_dict = self.constant_values()['TRADE_ACTION']\n",
    "        \n",
    "        # Close trade by Selling at Ask price, and Buying at Bid price\n",
    "        if closed_trade:\n",
    "            return bid if action == const_action_dict['BUY'] else ask\n",
    "        \n",
    "        # Open trade by Buying at Ask price, and Selling at Bid price\n",
    "        else:\n",
    "            return ask if action == const_action_dict['BUY'] else bid\n",
    "    \n",
    "    def __profit_by_action(self, entry_action, entry_price, curr_bid, curr_ask):\n",
    "        const_action_dict = self.constant_values()['TRADE_ACTION']\n",
    "        if entry_action == const_action_dict['BUY']:\n",
    "            return curr_ask - entry_price\n",
    "        \n",
    "        elif entry_action == const_action_dict['SELL']:\n",
    "            return entry_price - curr_bid\n",
    "        return 0\n",
    "    \n",
    "    def update_timestep(self, index):\n",
    "        try:\n",
    "            self.timestep = {\n",
    "                'index': self.indexes[index],\n",
    "                'datetime': self.datetimes[index],\n",
    "                'bid': self.bids[index],\n",
    "                'ask': self.asks[index]\n",
    "            }\n",
    "            return False\n",
    "        \n",
    "        except:\n",
    "            self.timestep = {}\n",
    "            return True\n",
    "    \n",
    "    def scale_state(self, state):\n",
    "        entry_action, bid_fluctuation_pct, ask_fluctuation_pct, equity_pct = state\n",
    "        \n",
    "        scaled_state      = min_max_scale(entry_action, -1, 2)\n",
    "        scaled_bid_pct    = min_max_scale(bid_fluctuation_pct, -100, 100)\n",
    "        scaled_ask_pct    = min_max_scale(ask_fluctuation_pct, -100, 100)\n",
    "        scaled_equity_pct = min_max_scale(equity_pct, -100, 100)\n",
    "        \n",
    "        return np.array([scaled_state, scaled_bid_pct, scaled_ask_pct, scaled_equity_pct])\n",
    "    \n",
    "    def reset(self, random=True):\n",
    "        # State\n",
    "        self.default_entry_action        = self.constant_values()['TRADE_ACTION']['DEFAULT']\n",
    "        self.default_bid_fluctuation_pct = 0.\n",
    "        self.default_ask_fluctuation_pct = 0.\n",
    "        self.default_equity_pct          = 100.\n",
    "        \n",
    "        entry_action        = self.default_entry_action\n",
    "        bid_fluctuation_pct = self.default_bid_fluctuation_pct\n",
    "        ask_fluctuation_pct = self.default_ask_fluctuation_pct\n",
    "        equity_pct          = self.default_equity_pct\n",
    "        self.observe_bid    = None\n",
    "        self.observe_ask    = None\n",
    "        self.state = np.array([entry_action, bid_fluctuation_pct, ask_fluctuation_pct, equity_pct])\n",
    "        \n",
    "        # Timestep\n",
    "        index = np.random.choice(self.indexes[:self.random_range]) if random else 0\n",
    "        self.update_timestep(index)\n",
    "        \n",
    "        # Trading\n",
    "        self.trading_params_dict = {\n",
    "            'orig_bal': 1_000_000.,\n",
    "            'acct_bal': 1_000_000.,\n",
    "            'unit':     100_000.,\n",
    "            \n",
    "            'trade_dict': {\n",
    "                'action':   [],\n",
    "                'datetime': [],\n",
    "                'price':    [],\n",
    "                'status':   [],\n",
    "                'profits':  [],\n",
    "                'acct_bal': []\n",
    "            }\n",
    "        }\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        const_action_dict = self.constant_values()['TRADE_ACTION']\n",
    "        const_status_dict = self.constant_values()['TRADE_STATUS']\n",
    "        \n",
    "        self.observe_bid = self.timestep['bid'] if self.observe_bid is None else self.observe_bid\n",
    "        self.observe_ask = self.timestep['ask'] if self.observe_ask is None else self.observe_ask\n",
    "        \n",
    "        trade_dict = self.trading_params_dict['trade_dict']\n",
    "        \n",
    "        # Get entry action & price\n",
    "        # - if there's no entry action, treat current action as action to open a trade\n",
    "        # - if there's entry action, treat current action as action to close a trade\n",
    "        try:\n",
    "            # NOTE: not to use pd.DataFrame() to convert trade_dict to dataframe, as it is slower\n",
    "            open_index      = trade_dict['status'].index(const_status_dict['OPEN'])\n",
    "            trade_actions   = trade_dict['action'][open_index:]\n",
    "            trade_prices    = trade_dict['price'][open_index:]\n",
    "            trade_datetimes = trade_dict['datetime'][open_index:]\n",
    "            \n",
    "            entry_action = trade_actions[0]\n",
    "            \n",
    "            # Not allowed to close open trades with same entry action\n",
    "            if entry_action == action:\n",
    "                trade_actions  = []\n",
    "                trade_prices   = []\n",
    "                trade_datetime = []\n",
    "            \n",
    "        except:\n",
    "            trade_actions  = []\n",
    "            trade_prices   = []\n",
    "            trade_datetime = []\n",
    "\n",
    "            entry_action = self.default_entry_action\n",
    "        \n",
    "        \n",
    "        profit = 0\n",
    "        closed_trade = False\n",
    "        sufficient_margin = True\n",
    "        if action in [const_action_dict['BUY'], const_action_dict['SELL']]:\n",
    "            # Close open trades\n",
    "            for trade_index, trade_price in enumerate(trade_prices):\n",
    "                profit += self.__profit_by_action(entry_action, trade_price, self.timestep['bid'], self.timestep['ask'])\n",
    "                profit *= self.trading_params_dict['unit']\n",
    "                profit = round(profit, 5)\n",
    "                \n",
    "                trade_dict['status'][trade_dict['datetime'].index(trade_datetimes[trade_index])] = const_status_dict['CLOSE']\n",
    "                closed_trade = True\n",
    "\n",
    "            # Add trade transaction\n",
    "            self.trading_params_dict['acct_bal'] += profit\n",
    "            price = self.__price_by_action(action, self.timestep['bid'], self.timestep['ask'], closed_trade)\n",
    "\n",
    "            # Add back free margin upon close trade\n",
    "            if closed_trade:\n",
    "                self.trading_params_dict['acct_bal'] += (len(trade_prices) * self.trading_params_dict['unit'])\n",
    "                \n",
    "            # Deduct required margin upon opening trade\n",
    "            else:\n",
    "                required_margin = self.trading_params_dict['unit']\n",
    "                if self.trading_params_dict['acct_bal'] < required_margin:\n",
    "                    sufficient_margin = False\n",
    "                self.trading_params_dict['acct_bal'] -= required_margin\n",
    "            \n",
    "            \n",
    "            trade_dict['action'].append(action)\n",
    "            trade_dict['datetime'].append(self.timestep['datetime'])\n",
    "            trade_dict['price'].append(price)\n",
    "            trade_dict['status'].append(const_status_dict['CLOSE_TRADE'] if closed_trade else const_status_dict['OPEN'])\n",
    "            trade_dict['profits'].append(profit)\n",
    "            trade_dict['acct_bal'].append(round(self.trading_params_dict['acct_bal'], 5))\n",
    "        \n",
    "        \n",
    "        # Calculate floating P/L\n",
    "        float_profit = 0\n",
    "        if (entry_action != self.default_entry_action) & (not closed_trade):\n",
    "            for trade_index, trade_price in enumerate(trade_prices):\n",
    "                float_profit = self.__profit_by_action(entry_action, trade_price, self.timestep['bid'], self.timestep['ask'])\n",
    "                float_profit *= self.trading_params_dict['unit']\n",
    "                float_profit = round(float_profit, 5)\n",
    "        \n",
    "        # Calculate equity %\n",
    "        equity_pct = (self.trading_params_dict['acct_bal'] + float_profit) / self.trading_params_dict['orig_bal'] * 100\n",
    "        equity_pct = round(equity_pct, 5)\n",
    "        \n",
    "        # Observe the price at current timestemp if open or closed trades, else observe the entry price\n",
    "        self.observe_bid = self.timestep['bid'] if closed_trade else self.observe_bid if action == const_action_dict['HOLD'] else self.timestep['bid']\n",
    "        self.observe_ask = self.timestep['ask'] if closed_trade else self.observe_ask if action == const_action_dict['HOLD'] else self.timestep['ask']\n",
    "        \n",
    "        # Calculate fluctuation %\n",
    "        bid_fluctuation_pct = round((self.timestep['bid'] - self.observe_bid) / self.observe_bid * 100, 5)\n",
    "        ask_fluctuation_pct = round((self.timestep['ask'] - self.observe_ask) / self.observe_ask * 100, 5)\n",
    "        \n",
    "        \n",
    "        # State\n",
    "        state_entry_action = self.default_entry_action if closed_trade else self.state[0] if action == const_action_dict['HOLD'] else action\n",
    "        next_state = np.array([state_entry_action, bid_fluctuation_pct, ask_fluctuation_pct, equity_pct])\n",
    "        self.state = next_state\n",
    "        \n",
    "        # Reward\n",
    "        reward = profit\n",
    "        \n",
    "        done = self.update_timestep(self.timestep['index'] +1)\n",
    "        if not done:\n",
    "            # Stop trading if do not have balance, and there's no open trade\n",
    "            if (self.trading_params_dict['acct_bal'] <= self.trading_params_dict['unit']) & (const_status_dict['OPEN'] not in trade_dict['status']):\n",
    "                done = True\n",
    "                \n",
    "            # Stop trading if do not have enough balance to pay for required margin\n",
    "            elif not sufficient_margin:\n",
    "                done = True\n",
    "        \n",
    "        # Additional information\n",
    "        info_dict = {\n",
    "            'closed_trade': closed_trade,\n",
    "            'sufficient_margin': sufficient_margin\n",
    "        }\n",
    "        return (self.state, reward, done, info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, memory_size, state_size, action_size):\n",
    "        self.memory_size = memory_size\n",
    "        self.counter     = 0\n",
    "        \n",
    "        self.states      = np.empty((0, state_size))\n",
    "        self.actions     = np.empty((0, action_size), dtype=np.int8)\n",
    "        self.rewards     = np.empty(0)\n",
    "        self.next_states = np.empty((0, state_size))\n",
    "        self.terminals   = np.empty(0, dtype=np.float32)\n",
    "        \n",
    "    def preprocess(self, experience):\n",
    "        state, action, reward, next_state, done = experience\n",
    "        \n",
    "        # One-Hot encoding\n",
    "        one_hot_action = np.zeros(self.actions.shape[1], dtype=np.int8)\n",
    "        one_hot_action[action] = 1\n",
    "        \n",
    "        return (state, one_hot_action, reward, next_state, (1 - done))\n",
    "    \n",
    "    # NOTE: not to use collections.deque, as it is much slower while perform sampling\n",
    "    def __deque(self, store_index, element, elements):\n",
    "        try:\n",
    "            elements[store_index] = element\n",
    "        except IndexError:\n",
    "            elements = np.append(elements, [element], axis=0)\n",
    "            \n",
    "        return elements\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        state, action, reward, next_state, terminal = self.preprocess((state, action, reward, next_state, done))\n",
    "        \n",
    "        store_index      = self.counter % self.memory_size\n",
    "        self.states      = self.__deque(store_index, state, self.states)\n",
    "        self.actions     = self.__deque(store_index, action, self.actions)\n",
    "        self.rewards     = self.__deque(store_index, reward, self.rewards)\n",
    "        self.next_states = self.__deque(store_index, next_state, self.next_states)\n",
    "        self.terminals   = self.__deque(store_index, terminal, self.terminals)\n",
    "        \n",
    "        self.counter += 1\n",
    "        \n",
    "    def currexp_to_sample(self, experience, experiences):\n",
    "        states, actions, rewards, next_states, terminals = experiences\n",
    "        \n",
    "        # Include current experience\n",
    "        if experience is not None:\n",
    "            state, action, reward, next_state, terminal = self.preprocess(experience)\n",
    "            \n",
    "            states      = np.vstack([states, state])\n",
    "            actions     = np.vstack([actions, action])\n",
    "            rewards     = np.append(rewards, reward)\n",
    "            next_states = np.vstack([next_states, next_state])\n",
    "            terminals   = np.append(terminals, terminal)\n",
    "            \n",
    "        return (states, actions, rewards, next_states, terminals)\n",
    "        \n",
    "    def sample(self, batch_size, experience=None):\n",
    "        batch_size  = batch_size if experience is None else batch_size -1\n",
    "        minibatch   = np.random.choice(len(self.states), batch_size)\n",
    "        \n",
    "        states      = self.states[minibatch]\n",
    "        actions     = self.actions[minibatch]\n",
    "        rewards     = self.rewards[minibatch]\n",
    "        next_states = self.next_states[minibatch]\n",
    "        terminals   = self.terminals[minibatch]\n",
    "        \n",
    "        return self.currexp_to_sample(experience, (states, actions, rewards, next_states, terminals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Sample Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSRBuffer(ReplayBuffer):\n",
    "    def __init__(self, memory_size, state_size, action_size, unusual_sample_factor=.99):\n",
    "        super().__init__(memory_size, state_size, action_size)\n",
    "        \n",
    "        # Reference: https://medium.com/ml-everything/reinforcement-learning-with-sparse-rewards-8f15b71d18bf\n",
    "        # Determine how much difference between experiences with low rewards and experiences with high rewards\n",
    "        # The lower the value, the higher then difference, and 1 means no different\n",
    "        self.unusual_sample_factor = unusual_sample_factor\n",
    "        \n",
    "    def sample(self, batch_size, experience=None):\n",
    "        # Sort rewards index descendingly by absolute value\n",
    "        sort_indexes = np.argsort(-np.abs(self.rewards))\n",
    "        \n",
    "        # Calculate probabilities and ensure sum of probabilities is max. 1\n",
    "        probabilities = np.power(self.unusual_sample_factor, np.arange(len(sort_indexes)))\n",
    "        probabilities = probabilities / np.sum(probabilities)\n",
    "        \n",
    "        # Sample minibatch\n",
    "        batch_size = batch_size if experience is None else batch_size -1\n",
    "        minibatch  = np.random.choice(np.arange(len(probabilities)), size=batch_size, p=probabilities)\n",
    "        minibatch_indexes = sort_indexes[minibatch]\n",
    "        \n",
    "        states      = self.states[minibatch_indexes]\n",
    "        actions     = self.actions[minibatch_indexes]\n",
    "        rewards     = self.rewards[minibatch_indexes]\n",
    "        next_states = self.next_states[minibatch_indexes]\n",
    "        terminals   = self.terminals[minibatch_indexes]\n",
    "        \n",
    "        return self.currexp_to_sample(experience, (states, actions, rewards, next_states, terminals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixModel:\n",
    "    def __init__(self, state_size, action_size, init_value=.0):\n",
    "        self.state_size  = state_size\n",
    "        self.action_size = action_size\n",
    "        self.init_value  = init_value\n",
    "        \n",
    "        self.states = np.empty((0, self.state_size))\n",
    "        self.values = np.empty((0, self.action_size))\n",
    "        \n",
    "    def state_values(self, state):\n",
    "        try:\n",
    "            index  = self.states.tolist().index(state.tolist())\n",
    "            values = self.values[index]\n",
    "        except:\n",
    "            self.states = np.append(self.states, [state], axis=0)\n",
    "            self.values = np.append(self.values, [[self.init_value for _ in range(self.action_size)]], axis=0)\n",
    "            values      = self.state_values(state)\n",
    "            \n",
    "        return values\n",
    "    \n",
    "    def state_action_value(self, state, action):\n",
    "        values = self.state_values(state)\n",
    "        return values[action]\n",
    "    \n",
    "    def set_state_action(self, state, action, value):\n",
    "        values = self.state_values(state)\n",
    "        values[action] = value\n",
    "        \n",
    "        # No need to update array as it's modified on referencing variable\n",
    "        # index = self.states.tolist().index(state.tolist())\n",
    "        # self.values[index] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialNetworkModel:\n",
    "    def __init__(self, state_size, action_size, alpha, neurons=[]):\n",
    "        # Input layer\n",
    "        inputs = Input(shape=(state_size,))\n",
    "        \n",
    "        # Hidden layer\n",
    "        for index, neuron in enumerate(neurons):\n",
    "            connected_layer = inputs if index == 0 else layer\n",
    "            layer = Dense(neuron, activation='relu', kernel_initializer='he_uniform')(connected_layer)\n",
    "        \n",
    "        # Output layer\n",
    "        connected_layer = inputs if len(neurons) == 0 else layer\n",
    "        outputs = Dense(action_size, activation='linear')(connected_layer)\n",
    "        \n",
    "        self.model = Model(inputs=inputs, outputs=outputs)\n",
    "        self.model.compile(optimizer=RMSprop(lr=alpha), loss='mse')\n",
    "        # Reference: https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4\n",
    "        # self.model.compile(optimizer=RMSprop(lr=alpha), loss=Huber(delta=1.0))\n",
    "        \n",
    "    def model_diagram(self, filename):\n",
    "        out_path = OUT_PATH_IMAGE\n",
    "        create_directory(out_path)\n",
    "        plot_model(self.model, to_file=f'{out_path}{filename}.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.hyperparams_dict = {\n",
    "            'epsilon': {\n",
    "                'min': np.nan, 'max': np.nan, 'decay': np.nan, 'value': np.nan\n",
    "            },\n",
    "            'alpha': {\n",
    "                'min': np.nan, 'max': np.nan, 'decay': np.nan, 'value': np.nan\n",
    "            },\n",
    "            'gamma': {\n",
    "                'min': np.nan, 'max': np.nan, 'increase': np.nan, 'value': np.nan\n",
    "            }\n",
    "        }\n",
    "        self.env = env\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        return random.choice(self.env.available_actions())\n",
    "    \n",
    "    def learn(self, experience, next_action, episode):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-Policy Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(Agent):\n",
    "    def __init__(self, env, build_model=True):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.hyperparams_dict = {\n",
    "            'epsilon': {\n",
    "                'min': .01, 'max': 1., 'decay': .001, 'value': 1.\n",
    "            },\n",
    "            'alpha': {\n",
    "                'min': .001, 'max': .9, 'decay': .001, 'value': .9\n",
    "            },\n",
    "            'gamma': {\n",
    "                'min': .9, 'max': .9, 'increase': .0, 'value': .9\n",
    "            }\n",
    "        }\n",
    "        self.env = env\n",
    "        if build_model:\n",
    "            self.main_model = self.build_model(env.state_size(), env.action_size())\n",
    "        \n",
    "    def build_model(self, state_size, action_size, init_value=.0):\n",
    "        return MatrixModel(state_size, action_size, init_value)\n",
    "    \n",
    "#     # TODO\n",
    "#     def save_model_checkpoint(self):\n",
    "#         out_path = OUT_PATH_FILE\n",
    "#         create_directory(out_path)\n",
    "#         self.main_model.save(f'{out_path}{self.model_file}')\n",
    "        \n",
    "#     def load_model_checkpoint(self):\n",
    "#         self.main_model = load_model(f'{OUT_PATH_FILE}{self.model_file}')\n",
    "    \n",
    "    def __random_argmax(self, q_values, random_if_empty=False):\n",
    "        # Reference:\n",
    "        # https://gist.github.com/stober/1943451\n",
    "        indexes = np.nonzero(q_values == np.amax(q_values))[0]\n",
    "        \n",
    "        try:\n",
    "            return np.random.choice(indexes)\n",
    "        except TypeOfError as error:\n",
    "            if not random_if_empty:\n",
    "                raise Exception(error)\n",
    "                \n",
    "            return np.random.choice(indexes if indexes.size > 0 else q_values.size)\n",
    "        \n",
    "    def exploitation(self, q_values):\n",
    "        # Validation whether action with highest Q-value is valid\n",
    "        actions      = self.env.action_space()\n",
    "        action_index = self.__random_argmax(q_values)\n",
    "        action       = actions[action_index]\n",
    "        \n",
    "        if action not in self.env.available_actions():\n",
    "            # Remove action if it's not a valid action on current state\n",
    "            q_values = np.delete(q_values, action_index)\n",
    "            del actions[action_index]\n",
    "\n",
    "            # Select action with 2nd highest Q-value\n",
    "            action_index = self.__random_argmax(q_values)\n",
    "            action       = actions[action_index]\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        # Exploration\n",
    "        if np.random.uniform(0, 1) <= self.hyperparams_dict['epsilon']['value']:\n",
    "            return random.choice(self.env.available_actions())\n",
    "        \n",
    "        # Exploitation\n",
    "        else:\n",
    "            q_values = self.main_model.state_values(state).copy()\n",
    "            return self.exploitation(q_values)\n",
    "        \n",
    "    def adjust_hyperparams(self, param_name, episode):\n",
    "        if param_name in ['epsilon', 'alpha']:\n",
    "            min_value = self.hyperparams_dict[param_name]['min']\n",
    "            max_value = self.hyperparams_dict[param_name]['max']\n",
    "            rate      = self.hyperparams_dict[param_name]['decay']\n",
    "        \n",
    "        elif param_name == 'gamma':\n",
    "            # Swap min. max. for incrementing\n",
    "            min_value = self.hyperparams_dict[param_name]['max']\n",
    "            max_value = self.hyperparams_dict[param_name]['min']\n",
    "            rate      = self.hyperparams_dict[param_name]['increase']\n",
    "            \n",
    "        self.hyperparams_dict[param_name]['value'] = min_value + (max_value - min_value) * np.exp(-rate * episode)\n",
    "        \n",
    "    def learn(self, experience, next_action, episode):\n",
    "        state, action, reward, next_state, done = experience\n",
    "        \n",
    "        # Q[s, a] = Q[s, a] + alpha * (reward + gamma * Max[Q(s’, A)] - Q[s, a])\n",
    "        max_q_value = np.max(self.main_model.state_values(next_state))\n",
    "        q_value = self.main_model.state_action_value(state, action)\n",
    "        q_value = q_value + self.hyperparams_dict['alpha']['value'] * (reward + self.hyperparams_dict['gamma']['value'] * max_q_value - q_value)\n",
    "        self.main_model.set_state_action(state, action, round(q_value, 10))\n",
    "        \n",
    "        # Adjust hyperparameters\n",
    "        if done:\n",
    "            self.adjust_hyperparams('epsilon', episode)\n",
    "            self.adjust_hyperparams('alpha', episode)\n",
    "            self.adjust_hyperparams('gamma', episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkAgent(QLearningAgent):\n",
    "    def __init__(self, env, build_model=True,\n",
    "                 sample_size=1, memory_size=1, model_file='QNetwork_MODEL.H5',\n",
    "                 neurons=[]):\n",
    "        \n",
    "        super().__init__(env, build_model=False)\n",
    "        \n",
    "        self.hyperparams_dict = {\n",
    "            'epsilon': {\n",
    "                'min': .01, 'max': 1., 'decay': .0005, 'value': 1.\n",
    "            },\n",
    "            'alpha': {\n",
    "                'min': .00025, 'max': .01, 'decay': .001, 'value': .01\n",
    "            },\n",
    "            'gamma': {\n",
    "                'min': .9, 'max': .9, 'increase': .0, 'value': .9\n",
    "            }\n",
    "        }\n",
    "        # self.memory       = ReplayBuffer(memory_size, self.env.state_size(), self.env.action_size())\n",
    "        self.memory       = WSRBuffer(memory_size, self.env.state_size(), self.env.action_size(),\n",
    "                                      unusual_sample_factor=.99)\n",
    "        self.sample_size  = sample_size\n",
    "        self.model_file   = model_file\n",
    "        \n",
    "        if build_model:\n",
    "            self.main_network = self.build_model(self.env.state_size(), self.env.action_size(),\n",
    "                                                 self.hyperparams_dict['alpha']['value'], neurons)\n",
    "            self.main_model   = self.main_network.model\n",
    "        \n",
    "    def build_model(self, state_size, action_size, alpha, neurons):\n",
    "        return SequentialNetworkModel(state_size, action_size, alpha, neurons=neurons)\n",
    "    \n",
    "    def save_model_checkpoint(self):\n",
    "        out_path = OUT_PATH_FILE\n",
    "        create_directory(out_path)\n",
    "        self.main_model.save(f'{out_path}{self.model_file}')\n",
    "        \n",
    "    def load_model_checkpoint(self):\n",
    "        self.main_model = load_model(f'{OUT_PATH_FILE}{self.model_file}')\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        # Exploration\n",
    "        if np.random.uniform(0, 1) <= self.hyperparams_dict['epsilon']['value']:\n",
    "            return random.choice(self.env.available_actions())\n",
    "        \n",
    "        # Exploitation\n",
    "        else:\n",
    "            # Change [obs1,obs2,obs3] to [[obs1,obs2,obs3]] format\n",
    "            state = state[np.newaxis, :]\n",
    "            \n",
    "            # Get Q-values for current state in [[q1,q2,q3]] format\n",
    "            q_values = self.main_model.predict(state, batch_size=len(state))\n",
    "            \n",
    "            # Change [[q1,q2,q3]] to [q1,q2,q3] format\n",
    "            q_values = q_values.reshape(-1)\n",
    "            \n",
    "            return self.exploitation(q_values)\n",
    "        \n",
    "    def learn(self, experience, next_action, episode):\n",
    "        state, action, reward, next_state, done = experience\n",
    "        self.memory.store_transition(state, action, reward, next_state, done)\n",
    "        \n",
    "        if self.memory.counter < self.sample_size:\n",
    "            return False\n",
    "        \n",
    "        # states:      [[obs1,obs2,obs3],[obs1,obs2,obs3]...[obs1,obs2,obs3]]\n",
    "        # actions:     [[0,0,1],[0,1,0]...[1,0,0]]\n",
    "        # rewards:     [r1,r2...rN]\n",
    "        # next_states: [[obs1,obs2,obs3],[obs1,obs2,obs3]...[obs1,obs2,obs3]]\n",
    "        # terminals:   [t1,t2...tN]\n",
    "        states, actions, rewards, next_states, terminals = self.memory.sample(self.sample_size, experience=experience)\n",
    "        \n",
    "        # Change actions from [[0,0,1],[0,1,0]...[1,0,0]] to [2,1...0] format\n",
    "        action_values  = np.array(self.env.action_space(), dtype=np.int8)\n",
    "        action_indexes = np.dot(actions, action_values)\n",
    "        \n",
    "        # Reference: https://www.youtube.com/watch?v=5fHngyN8Qhw\n",
    "        # Get Q-values for states in [[q1,q2,q3],[q1,q2,q3]...[q1,q2,q3]] format\n",
    "        q_values      = self.main_model.predict(states, batch_size=len(states))\n",
    "        next_q_values = self.main_model.predict(next_states, batch_size=len(next_states))\n",
    "        q_targets     = q_values.copy()\n",
    "        \n",
    "        # Calculate Q-targets\n",
    "        sample_indexes = np.arange(self.sample_size, dtype=np.int32)\n",
    "        q_targets[sample_indexes, action_indexes] = rewards + self.hyperparams_dict['gamma']['value'] * np.max(next_q_values, axis=1)\n",
    "        \n",
    "        # Update Q-targets\n",
    "        self.main_model.fit(states, q_targets, epochs=1, verbose=0, batch_size=len(states))\n",
    "        \n",
    "        # Adjust hyperparameters\n",
    "        if done:\n",
    "            self.adjust_hyperparams('epsilon', episode)\n",
    "            self.adjust_hyperparams('alpha', episode)\n",
    "            self.adjust_hyperparams('gamma', episode)\n",
    "            \n",
    "            # Update optimizer learning rate\n",
    "            K.eval(self.main_model.optimizer.lr.assign(self.hyperparams_dict['alpha']['value']))\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(QNetworkAgent):\n",
    "    def __init__(self, env,\n",
    "                 sample_size=1, memory_size=1, model_file='DQN_MODEL.H5',\n",
    "                 neurons=[1_024, 512, 256]):\n",
    "        \n",
    "        super().__init__(env, sample_size=sample_size, memory_size=memory_size, model_file=model_file,\n",
    "                         neurons=neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent(DQNAgent):\n",
    "    def __init__(self, env,\n",
    "                 sample_size=1, memory_size=1, model_file='DoubleDQN_MODEL.H5',\n",
    "                 neurons=[1_024, 512, 256]):\n",
    "        \n",
    "        super().__init__(env, sample_size=sample_size, memory_size=memory_size, model_file=model_file,\n",
    "                         neurons=neurons)\n",
    "        \n",
    "        # Target Network\n",
    "        self.hyperparams_dict['tau'] = .125\n",
    "        self.target_network = self.build_model(self.env.state_size(), self.env.action_size(),\n",
    "                                               self.hyperparams_dict['alpha']['value'], neurons)\n",
    "        self.target_model   = self.target_network.model\n",
    "        self.target_model.set_weights(self.main_model.get_weights())\n",
    "        \n",
    "    def learn(self, experience, next_action, episode):\n",
    "        state, action, reward, next_state, done = experience\n",
    "        self.memory.store_transition(state, action, reward, next_state, done)\n",
    "        \n",
    "        if self.memory.counter < self.sample_size:\n",
    "            return False\n",
    "        \n",
    "        # states:      [[obs1,obs2,obs3],[obs1,obs2,obs3]...[obs1,obs2,obs3]]\n",
    "        # actions:     [[0,0,1],[0,1,0]...[1,0,0]]\n",
    "        # rewards:     [r1,r2...rN]\n",
    "        # next_states: [[obs1,obs2,obs3],[obs1,obs2,obs3]...[obs1,obs2,obs3]]\n",
    "        # terminals:   [t1,t2...tN]\n",
    "        states, actions, rewards, next_states, terminals = self.memory.sample(self.sample_size, experience=experience)\n",
    "        \n",
    "        # Change actions from [[0,0,1],[0,1,0]...[1,0,0]] to [2,1...0] format\n",
    "        action_values  = np.array(self.env.action_space(), dtype=np.int8)\n",
    "        action_indexes = np.dot(actions, action_values)\n",
    "        \n",
    "        # Fixed Q-Target\n",
    "        # Reference: https://www.youtube.com/watch?v=UCgsv6tMReY\n",
    "        # Get Q-values for states in [[q1,q2,q3],[q1,q2,q3]...[q1,q2,q3]] format\n",
    "        q_values      = self.target_model.predict(next_states, batch_size=len(next_states))\n",
    "        next_q_values = self.main_model.predict(next_states, batch_size=len(next_states))\n",
    "        q_targets     = self.main_model.predict(states, batch_size=len(states))\n",
    "        \n",
    "        # Calculate Q-targets\n",
    "        max_q_indexes  = np.argmax(next_q_values, axis=1)\n",
    "        sample_indexes = np.arange(self.sample_size, dtype=np.int32)\n",
    "        q_targets[sample_indexes, action_indexes] = rewards + self.hyperparams_dict['gamma']['value'] * q_values[sample_indexes, max_q_indexes.astype(int)]\n",
    "        \n",
    "        # Update Q-targets\n",
    "        self.main_model.fit(states, q_targets, epochs=1, verbose=0, batch_size=len(states))\n",
    "        \n",
    "        # Update target network weights\n",
    "        # Reference: https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c\n",
    "        main_weights   = self.main_model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for weight_index in range(len(target_weights)):\n",
    "            target_weights[weight_index] = main_weights[weight_index] * self.hyperparams_dict['tau'] + target_weights[weight_index] * (1 - self.hyperparams_dict['tau'])\n",
    "        self.target_model.set_weights(target_weights)\n",
    "        \n",
    "        # Adjust hyperparameters\n",
    "        if done:\n",
    "            self.adjust_hyperparams('epsilon', episode)\n",
    "            self.adjust_hyperparams('alpha', episode)\n",
    "            self.adjust_hyperparams('gamma', episode)\n",
    "            \n",
    "            # Update optimizer learning rate\n",
    "            K.eval(self.main_model.optimizer.lr.assign(self.hyperparams_dict['alpha']['value']))\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dueling Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-Policy Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent(QLearningAgent):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def learn(self, experience, next_action, episode):\n",
    "        state, action, reward, next_state, done = experience\n",
    "        \n",
    "        # Q[s, a] = Q[s, a] + alpha * (reward + gamma * Q(s’, a’) - Q[s, a])\n",
    "        next_q_value = self.main_model.state_action_value(next_state, next_action)\n",
    "        q_value = self.main_model.state_action_value(state, action)\n",
    "        q_value = q_value + self.hyperparams_dict['alpha']['value'] * (reward + self.hyperparams_dict['gamma']['value'] * next_q_value - q_value)\n",
    "        self.main_model.set_state_action(state, action, round(q_value, 10))\n",
    "        \n",
    "        # Adjust hyperparameters\n",
    "        if done:\n",
    "            self.adjust_hyperparams('epsilon', episode)\n",
    "            self.adjust_hyperparams('alpha', episode)\n",
    "            self.adjust_hyperparams('gamma', episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA (λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaLambdaAgent(SarsaAgent):\n",
    "    def __init__(self, env, episodic_trace=False):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        # Eligibility Trace\n",
    "        self.hyperparams_dict['elig_trace'] = {\n",
    "            'init': 1 / env.action_size(),\n",
    "            'lambda': .9\n",
    "        }\n",
    "        self.e_model        = self.__build_e_model()\n",
    "        self.episodic_trace = episodic_trace\n",
    "        \n",
    "    def __build_e_model(self):\n",
    "        return self.build_model(env.state_size(), env.action_size(),\n",
    "                                init_value=self.hyperparams_dict['elig_trace']['init'])\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        # Exploration\n",
    "        if np.random.uniform(0, 1) <= self.hyperparams_dict['epsilon']['value']:\n",
    "            return random.choice(self.env.available_actions())\n",
    "        \n",
    "        # Exploitation\n",
    "        else:\n",
    "            q_values = self.main_model.state_values(state).copy()\n",
    "            # Ensure state added to main model is added to elibility trace as well\n",
    "            self.e_model.state_values(state)\n",
    "            return self.exploitation(q_values)\n",
    "        \n",
    "    def learn(self, experience, next_action, episode):\n",
    "        state, action, reward, next_state, done = experience\n",
    "        \n",
    "        # Reference:\n",
    "        # https://naifmehanna.com/2018-10-18-implementing-sarsa-in-python/\n",
    "        next_q_value = self.main_model.state_action_value(next_state, next_action)\n",
    "        q_value = self.main_model.state_action_value(state, action)\n",
    "        \n",
    "        # Ensure state added to main model is added to elibility trace as well\n",
    "        self.e_model.state_action_value(next_state, next_action)\n",
    "        e_value = self.e_model.state_action_value(state, action)\n",
    "        self.e_model.set_state_action(state, action, e_value +1)\n",
    "        \n",
    "        states_q_values = self.main_model.values\n",
    "        states_e_values = self.e_model.values\n",
    "        \n",
    "        # Calculate & Update Q-matrix\n",
    "        states_q_values = states_q_values + self.hyperparams_dict['alpha']['value'] * (reward + self.hyperparams_dict['gamma']['value'] * next_q_value - q_value) * states_e_values\n",
    "        self.main_model.values = np.round(states_q_values, 10)\n",
    "        \n",
    "        # Decay & Update E-matrix\n",
    "        states_e_values = states_e_values * self.hyperparams_dict['gamma']['value'] * self.hyperparams_dict['elig_trace']['lambda']\n",
    "        self.e_model.values = np.round(states_e_values, 10)\n",
    "        \n",
    "        # Adjust hyperparameters\n",
    "        if done:\n",
    "            self.adjust_hyperparams('epsilon', episode)\n",
    "            self.adjust_hyperparams('alpha', episode)\n",
    "            self.adjust_hyperparams('gamma', episode)\n",
    "            \n",
    "            # Re-initialize Eligibility Trace on each episode:\n",
    "            # https://stackoverflow.com/questions/29904270/eligibility-trace-reinitialization-between-episodes-in-sarsa-lambda-implementati\n",
    "            if self.episodic_trace:\n",
    "                self.e_model = self.__build_e_model()\n",
    "                self.e_model.states = self.main_model.states.copy()\n",
    "                self.e_model.values = np.full(self.e_model.states.shape, self.hyperparams_dict['elig_trace']['init'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_pair = 'EURUSD'\n",
    "filename      = f'DAT_ASCII_{currency_pair}_T_201901.csv'\n",
    "\n",
    "env = ForexEnv(SOURCE_PATH_DATA, filename, nrows=200, train_size=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "price_types = ['bid', 'ask']\n",
    "for price_index, prices in enumerate([env.bids, env.asks]):\n",
    "    data.append(go.Scattergl(\n",
    "        x = pd.DataFrame(env.datetimes)[0],\n",
    "        y = prices,\n",
    "        mode = 'lines',\n",
    "        name = price_types[price_index].title()\n",
    "    ))\n",
    "\n",
    "title = f'{currency_pair} - Forex Environment'\n",
    "plot_graph(data, title, 'Date Time', 'Price')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes, agent_type):\n",
    "    try:\n",
    "        sample_size = 1_000\n",
    "        memory_size = 1_000_000\n",
    "        neurons=[128]\n",
    "\n",
    "        # Off-Policy agent\n",
    "        if agent_type == 'Normal':\n",
    "            agent = Agent(env)\n",
    "\n",
    "        elif agent_type == 'Q-Learning':\n",
    "            agent = QLearningAgent(env)\n",
    "\n",
    "        elif agent_type == 'Q-Network':\n",
    "            agent = QNetworkAgent(env, sample_size=sample_size, memory_size=memory_size)\n",
    "            agent.main_network.model_diagram(agent_type)\n",
    "\n",
    "        elif agent_type == 'DQN':\n",
    "            agent = DQNAgent(env, sample_size=sample_size, memory_size=memory_size, neurons=neurons)\n",
    "            agent.main_network.model_diagram(agent_type)\n",
    "\n",
    "        elif agent_type == 'Double DQN':\n",
    "            agent = DoubleDQNAgent(env, sample_size=sample_size, memory_size=memory_size, neurons=neurons)\n",
    "            agent.main_network.model_diagram(agent_type)\n",
    "\n",
    "        # On-Policy agent\n",
    "        elif agent_type == 'SARSA':\n",
    "            agent = SarsaAgent(env)\n",
    "\n",
    "        elif agent_type == 'SARSA Lambda':\n",
    "            agent = SarsaLambdaAgent(env, episodic_trace=False)\n",
    "\n",
    "\n",
    "        # Performance tracking\n",
    "        result_dict = {\n",
    "            'total_profit': [],\n",
    "            'used_margin': [],\n",
    "            'acct_bal': [],\n",
    "            'trades': []\n",
    "        }\n",
    "\n",
    "        # Training iteration\n",
    "        for episode in range(episodes):\n",
    "            # Walkthrough environment\n",
    "            done  = False\n",
    "            state = env.reset()\n",
    "\n",
    "            while not done:\n",
    "                # Choose action\n",
    "                # scaled_state = state\n",
    "                scaled_state  = env.scale_state(state)\n",
    "                action = agent.choose_action(scaled_state)\n",
    "                \n",
    "                # Take action\n",
    "                next_state, reward, done, info_dict = env.step(action)\n",
    "\n",
    "                # Choose next action\n",
    "                # scaled_next_state = next_state\n",
    "                scaled_next_state = env.scale_state(next_state)\n",
    "                next_action = agent.choose_action(scaled_next_state)\n",
    "\n",
    "                # Learning\n",
    "                scaled_reward = reward\n",
    "                # scaled_reward = zero_one_scale(reward)\n",
    "                agent.learn((scaled_state, action, scaled_reward, scaled_next_state, done), next_action, episode)\n",
    "\n",
    "                state  = next_state\n",
    "                action = next_action\n",
    "\n",
    "            # Result Summary\n",
    "            trade_df      = pd.DataFrame(env.trading_params_dict['trade_dict'])\n",
    "            total_profits = sum(trade_df['profits'])\n",
    "\n",
    "            open_trade_df = trade_df[trade_df['status'].isin([env.constant_values()['TRADE_STATUS']['OPEN']])]\n",
    "            used_margin   = sum(open_trade_df['price'] * env.trading_params_dict['unit'])\n",
    "\n",
    "            result_dict['total_profit'].append(round(total_profits, 5))\n",
    "            result_dict['used_margin'].append(round(used_margin, 5))\n",
    "            result_dict['acct_bal'].append(round(env.trading_params_dict['acct_bal'], 5))\n",
    "            result_dict['trades'].append(env.trading_params_dict['trade_dict'])\n",
    "\n",
    "            # Progress\n",
    "            # clear_output(wait=True)\n",
    "            ε = agent.hyperparams_dict['epsilon']['value']\n",
    "            α = agent.hyperparams_dict['alpha']['value']\n",
    "            γ = agent.hyperparams_dict['gamma']['value']\n",
    "            \n",
    "            try:\n",
    "                print(f'EP: {episode+1 :,} / {episodes :,} | ε: {ε :.5f} | α: {α :.5f} | γ: {γ :.5f} | R: {total_profits :,.5f} | M: {agent.memory.counter :,}')\n",
    "            except:\n",
    "                print(f'EP: {episode+1 :,} / {episodes :,} | ε: {ε :.5f} | α: {α :.5f} | γ: {γ :.5f} | R: {total_profits :,.5f}')\n",
    "\n",
    "        return result_dict, agent\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print('\\n!!! KeyboardInterrupt Exception !!!')\n",
    "        return result_dict, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "# agent_type = 'Normal'\n",
    "agent_type = 'Q-Learning'\n",
    "# agent_type = 'SARSA'\n",
    "# agent_type = 'SARSA Lambda'\n",
    "# agent_type = 'Q-Network'\n",
    "# agent_type = 'DQN'\n",
    "# agent_type = 'Double DQN'\n",
    "\n",
    "episodes = 5_000\n",
    "# episodes = 3_000\n",
    "\n",
    "# FOR PROFILING PURPOSE\n",
    "# %lprun -f train \\\n",
    "result_dict, agent = train(episodes, agent_type)\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\n",
    "    'total_profit': result_dict['total_profit'],\n",
    "    'acct_bal': result_dict['acct_bal'],\n",
    "    'used_margin': result_dict['used_margin']\n",
    "})\n",
    "result_df['equity'] = result_df['acct_bal'] + result_df['used_margin']\n",
    "\n",
    "# trade_df = pd.DataFrame(result_dict['trades'][0])\n",
    "# trade_df = pd.DataFrame(result_dict['trades'][len(result_dict['trades']) -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Rolling Profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x+1 for x in range(episodes)],\n",
    "    y = result_df.rolling(100).mean()['total_profit'],\n",
    "    mode = 'lines',\n",
    "    name = 'Rolling Profits'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Rolling Profits - {agent_type}'\n",
    "plot_graph(data, title, 'Episode', 'Amount')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Rolling Equity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x+1 for x in range(episodes)],\n",
    "    y = result_df.rolling(100).mean()['equity'],\n",
    "    mode = 'lines',\n",
    "    name = 'Equity'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Rolling Equity - {agent_type}'\n",
    "plot_graph(data, title, 'Episode', 'Equity')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "data = []\n",
    "price_types = ['bid', 'ask']\n",
    "for price_index, prices in enumerate([env.bids, env.asks]):\n",
    "    data.append(go.Scattergl(\n",
    "        x = pd.DataFrame(env.datetimes)[0],\n",
    "        y = prices,\n",
    "        mode = 'lines',\n",
    "        name = price_types[price_index].title(),\n",
    "        \n",
    "        # Additional settings\n",
    "        hoverinfo='skip'\n",
    "    ))\n",
    "\n",
    "markers = ['triangle-up', 'triangle-down']\n",
    "trade_actions = ['buy', 'sell']\n",
    "for trade_index, trade_action in enumerate(trade_actions):\n",
    "    action_df = trade_df[trade_df['action'] == trade_index]\n",
    "    \n",
    "    data.append(go.Scattergl(\n",
    "        x = action_df['datetime'],\n",
    "        y = action_df['price'],\n",
    "        mode = 'markers',\n",
    "        name = trade_action.title(),\n",
    "        \n",
    "        # Additional settings\n",
    "        marker = dict(\n",
    "            size=15,\n",
    "            symbol=markers[trade_index]\n",
    "        ),\n",
    "        hovertext=[f'Date Time: {row.datetime}<br />Action Index: {row.Index}<br />{\"Open\" if row.Index % 2 == 0 else \"Closed\"} at {row.price}<br />Profit: {row.profits}'\n",
    "                   for row in action_df.itertuples()],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "title = f'{currency_pair} - Trade - {agent_type}'\n",
    "plot_graph(data, title, 'Date Time', 'Price')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Trade Profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "\n",
    "y = trade_df['profits']\n",
    "\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    x = [x for x in range(len(trade_df))],\n",
    "    y = y,\n",
    "    mode = 'lines',\n",
    "    name = f'Profits ({sum(y) :,.2f})'\n",
    "))\n",
    "\n",
    "title = f'{currency_pair} - Trade Profits - {agent_type}'\n",
    "plot_graph(data, title, '', 'Amount')\n",
    "\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
