{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib._util.visualplot as vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Time measurement\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# Sound notification\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH_DATA = 'resources/data/'\n",
    "OUT_PATH_GRAPH = 'resources/output/data_preparation/graph/'\n",
    "OUT_PATH_FILE = 'resources/output/data_preparation/file/'\n",
    "\n",
    "def time_taken(seconds):\n",
    "    print(f'\\nTime Taken: {str(timedelta(seconds=seconds))}')\n",
    "    winsound.Beep(frequency=1000, duration=100)\n",
    "    winsound.Beep(frequency=1500, duration=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation (Raw Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(currency_pair, periods):\n",
    "    df_list = []\n",
    "    for period in periods:\n",
    "        source_file = f'resources/data/DAT_ASCII_{currency_pair}_T_{period}.csv'\n",
    "        df_chunks   = pd.read_csv(source_file, sep=',',\n",
    "                                  header=None, names=['datetime', 'bid', 'ask', 'vol'],\n",
    "                                  usecols=['datetime', 'bid', 'ask'],\n",
    "                                  parse_dates=['datetime'],\n",
    "                                  date_parser=lambda x: pd.to_datetime(x, format='%Y%m%d %H%M%S%f'),\n",
    "                                  chunksize=50_000)\n",
    "\n",
    "        df = pd.concat(df_chunks)\n",
    "        df_list.append(df)\n",
    "\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "currency_pair = 'EURUSD'\n",
    "periods       = [f'2019{str(x+1).zfill(2)}' for x in range(6)]\n",
    "\n",
    "timeseries_df = load_data(currency_pair, periods)\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.fast_stat(timeseries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "- Timeframe aggregation\n",
    "- Calculate oscillators\n",
    "  1. RSI\n",
    "  2. Stochastic RSI\n",
    "  3. Fast & Slow Stochastic\n",
    "  4. MACD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(df, rule):\n",
    "    # Resampling\n",
    "    bid_df = df.set_index('datetime')['bid'].resample(rule).ohlc().reset_index()\n",
    "    ask_df = df.set_index('datetime')['ask'].resample(rule).ohlc().reset_index()\n",
    "\n",
    "    bid_df.dropna(inplace=True)\n",
    "    ask_df.dropna(inplace=True)\n",
    "\n",
    "    bid_df.reset_index(drop=True, inplace=True)\n",
    "    ask_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    new_df = pd.DataFrame({\n",
    "        'datetime': bid_df['datetime'],\n",
    "\n",
    "        'open_bid': bid_df['open'],\n",
    "        'high_bid': bid_df['high'],\n",
    "        'low_bid': bid_df['low'],\n",
    "        'bid': bid_df['close'],\n",
    "\n",
    "        'open_ask': ask_df['open'],\n",
    "        'high_ask': ask_df['high'],\n",
    "        'low_ask': ask_df['low'],\n",
    "        'ask': ask_df['close']\n",
    "    })\n",
    "    new_df['datetime'] = new_df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Oscillator\n",
    "    n_timestep = 14\n",
    "    new_df = calc_roc(new_df, n_timestep)\n",
    "    new_df = calc_rsi(new_df, n_timestep)\n",
    "    new_df = calc_stochastic_rsi(new_df, n_timestep)\n",
    "    new_df = calc_stochastic(new_df, n_timestep, slow_timestep=3)\n",
    "    new_df = calc_macd(new_df)\n",
    "    \n",
    "    # Decimal rounding\n",
    "    for column in [x for x in new_df.columns if x != 'datetime']:\n",
    "        new_df[column] = np.round(new_df[column], 5)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# - https://www.youtube.com/watch?v=Gbnhbr4HhG0\n",
    "# - https://www.marketvolume.com/technicalanalysis/roc.asp\n",
    "def calc_roc(df, n_timestep):\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    for row in new_df[n_timestep -1:].itertuples():\n",
    "        bid_nperiod = new_df.at[row.Index - (n_timestep -1), 'bid']\n",
    "        ask_nperiod = new_df.at[row.Index - (n_timestep -1), 'ask']\n",
    "        \n",
    "        new_df.at[row.Index, 'bid_roc'] = (row.bid - bid_nperiod) / bid_nperiod * 100\n",
    "        new_df.at[row.Index, 'ask_roc'] = (row.ask - ask_nperiod) / ask_nperiod * 100\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# - https://www.youtube.com/watch?v=WZbOeFsSirM\n",
    "def calc_rsi(df, n_timestep):\n",
    "    new_df = df.copy()\n",
    "    tmp_df = df.copy()\n",
    "    \n",
    "    tmp_df['bid_movement'] = tmp_df['bid'].diff()\n",
    "    tmp_df['ask_movement'] = tmp_df['ask'].diff()\n",
    "\n",
    "    tmp_df['bid_upward_movement']   = np.where(tmp_df['bid_movement'] > 0, tmp_df['bid_movement'], 0)\n",
    "    tmp_df['bid_downward_movement'] = np.where(tmp_df['bid_movement'] < 0, np.abs(tmp_df['bid_movement']), 0)\n",
    "\n",
    "    tmp_df['ask_upward_movement']   = np.where(tmp_df['ask_movement'] > 0, tmp_df['ask_movement'], 0)\n",
    "    tmp_df['ask_downward_movement'] = np.where(tmp_df['ask_movement'] < 0, np.abs(tmp_df['ask_movement']), 0)\n",
    "\n",
    "    tmp_df.at[n_timestep -1, 'bid_avg_upward_movement']   = tmp_df[['bid_upward_movement']][:n_timestep].values.mean()\n",
    "    tmp_df.at[n_timestep -1, 'bid_avg_downward_movement'] = tmp_df[['bid_downward_movement']][:n_timestep].values.mean()\n",
    "\n",
    "    tmp_df.at[n_timestep -1, 'ask_avg_upward_movement']   = tmp_df[['ask_upward_movement']][:n_timestep].values.mean()\n",
    "    tmp_df.at[n_timestep -1, 'ask_avg_downward_movement'] = tmp_df[['ask_downward_movement']][:n_timestep].values.mean()\n",
    "\n",
    "    tmp_df = tmp_df[n_timestep -1:].copy()\n",
    "    tmp_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    for row in tmp_df[1:].itertuples():\n",
    "        tmp_df.at[row.Index, 'bid_avg_upward_movement']   = (tmp_df.at[row.Index -1, 'bid_avg_upward_movement'] * (n_timestep -1) + row.bid_upward_movement) / n_timestep\n",
    "        tmp_df.at[row.Index, 'bid_avg_downward_movement'] = (tmp_df.at[row.Index -1, 'bid_avg_downward_movement'] * (n_timestep -1) + row.bid_downward_movement) / n_timestep\n",
    "\n",
    "        tmp_df.at[row.Index, 'ask_avg_upward_movement']   = (tmp_df.at[row.Index -1, 'ask_avg_upward_movement'] * (n_timestep -1) + row.ask_upward_movement) / n_timestep\n",
    "        tmp_df.at[row.Index, 'ask_avg_downward_movement'] = (tmp_df.at[row.Index -1, 'ask_avg_downward_movement'] * (n_timestep -1) + row.ask_downward_movement) / n_timestep\n",
    "\n",
    "    tmp_df['bid_relative_strength'] = tmp_df['bid_avg_upward_movement'] / tmp_df['bid_avg_downward_movement']\n",
    "    tmp_df['ask_relative_strength'] = tmp_df['ask_avg_upward_movement'] / tmp_df['ask_avg_downward_movement']\n",
    "\n",
    "    tmp_df['bid_rsi'] = 100 - (100 / (tmp_df['bid_relative_strength'] + 1))\n",
    "    tmp_df['ask_rsi'] = 100 - (100 / (tmp_df['ask_relative_strength'] + 1))\n",
    "\n",
    "    tmp_df = tmp_df[['datetime', 'bid_rsi', 'ask_rsi']]\n",
    "    \n",
    "    return new_df.merge(tmp_df, on='datetime', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# - https://www.youtube.com/watch?v=cGDUQCCELMo\n",
    "# - https://www1.oanda.com/forex-trading/learn/trading-tools-strategies/stochastic\n",
    "def calc_stochastic_rsi(df, n_timestep):\n",
    "    new_df = df.copy()\n",
    "    tmp_df = new_df[new_df['bid_rsi'].isna() == False].copy()\n",
    "    tmp_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    tmp_df['roll_high_bid_rsi'] = tmp_df['bid_rsi'].rolling(n_timestep).max()\n",
    "    tmp_df['roll_high_ask_rsi'] = tmp_df['ask_rsi'].rolling(n_timestep).max()\n",
    "    tmp_df['roll_low_bid_rsi']  = tmp_df['bid_rsi'].rolling(n_timestep).min()\n",
    "    tmp_df['roll_low_ask_rsi']  = tmp_df['ask_rsi'].rolling(n_timestep).min()\n",
    "    \n",
    "    tmp_df.dropna().reset_index(inplace=True, drop=True)\n",
    "    tmp_df['bid_stochastic_rsi'] = (tmp_df['bid_rsi'] - tmp_df['roll_low_bid_rsi']) / (tmp_df['roll_high_bid_rsi'] - tmp_df['roll_low_bid_rsi']) * 100\n",
    "    tmp_df['ask_stochastic_rsi'] = (tmp_df['ask_rsi'] - tmp_df['roll_low_ask_rsi']) / (tmp_df['roll_high_ask_rsi'] - tmp_df['roll_low_ask_rsi']) * 100\n",
    "    \n",
    "    tmp_df = tmp_df[['datetime', 'bid_stochastic_rsi', 'ask_stochastic_rsi']]\n",
    "    \n",
    "    return new_df.merge(tmp_df, on='datetime', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# - https://www.youtube.com/watch?v=1UaPhm-TIkw\n",
    "# - https://www.investopedia.com/ask/answers/05/062405.asp\n",
    "# - https://www1.oanda.com/forex-trading/learn/trading-tools-strategies/stochastic\n",
    "def calc_stochastic(df, n_timestep, slow_timestep):\n",
    "    new_df = df.copy()\n",
    "    tmp_df = df.copy()\n",
    "    \n",
    "    tmp_df['roll_high_bid'] = tmp_df['bid'].rolling(n_timestep).max()\n",
    "    tmp_df['roll_high_ask'] = tmp_df['ask'].rolling(n_timestep).max()\n",
    "    tmp_df['roll_low_bid']  = tmp_df['bid'].rolling(n_timestep).min()\n",
    "    tmp_df['roll_low_ask']  = tmp_df['ask'].rolling(n_timestep).min()\n",
    "    \n",
    "    tmp_df = tmp_df[tmp_df['roll_high_bid'].isna() == False].copy()\n",
    "    tmp_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    tmp_df['bid-rlb'] = tmp_df['bid'] - tmp_df['roll_low_bid']\n",
    "    tmp_df['rhb-rlb'] = tmp_df['roll_high_bid'] - tmp_df['roll_low_bid']\n",
    "    tmp_df['ask-rla'] = tmp_df['ask'] - tmp_df['roll_low_ask']\n",
    "    tmp_df['rha-rla'] = tmp_df['roll_high_ask'] - tmp_df['roll_low_ask']\n",
    "    \n",
    "    tmp_df['bid_fast_stochastic'] = tmp_df['bid-rlb'] / tmp_df['rhb-rlb'] * 100\n",
    "    tmp_df['ask_fast_stochastic'] = tmp_df['ask-rla'] / tmp_df['rha-rla'] * 100\n",
    "    \n",
    "    tmp_df['bid_slow_stochastic'] = tmp_df['bid-rlb'].rolling(slow_timestep).sum() / tmp_df['rhb-rlb'].rolling(slow_timestep).sum() * 100\n",
    "    tmp_df['ask_slow_stochastic'] = tmp_df['ask-rla'].rolling(slow_timestep).sum() / tmp_df['rha-rla'].rolling(slow_timestep).sum() * 100\n",
    "    \n",
    "    tmp_df = tmp_df[['datetime', 'bid_fast_stochastic', 'ask_fast_stochastic', 'bid_slow_stochastic', 'ask_slow_stochastic']]\n",
    "    \n",
    "    return new_df.merge(tmp_df, on='datetime', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# - https://www.youtube.com/watch?v=so0NMh67ySw\n",
    "# - https://www.investopedia.com/terms/m/macd.asp\n",
    "def calc_macd(df):\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    fast_ema_timestep = 12\n",
    "    slow_ema_timestep = 26\n",
    "    signal_timestep   = 9\n",
    "    \n",
    "    fast_ema_factor = 2 / (fast_ema_timestep +1)\n",
    "    slow_ema_factor = 2 / (slow_ema_timestep +1)\n",
    "    signal_factor   = 2 / (signal_timestep +1)\n",
    "    \n",
    "    # Fast EMA\n",
    "    tmp_df = df.copy()\n",
    "    tmp_df.at[fast_ema_timestep -1, 'bid_fast_ema'] = tmp_df[['bid']][:fast_ema_timestep].values.mean()\n",
    "    tmp_df.at[fast_ema_timestep -1, 'ask_fast_ema'] = tmp_df[['ask']][:fast_ema_timestep].values.mean()\n",
    "    \n",
    "    tmp_df = tmp_df[fast_ema_timestep -1:].copy()\n",
    "    tmp_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    for row in tmp_df[1:].itertuples():\n",
    "        prev_bfema = tmp_df.at[row.Index -1, 'bid_fast_ema']\n",
    "        prev_afema = tmp_df.at[row.Index -1, 'ask_fast_ema']\n",
    "        \n",
    "        tmp_df.at[row.Index, 'bid_fast_ema'] = ((row.bid - tmp_df.at[row.Index -1, 'bid_fast_ema']) * fast_ema_factor) + prev_bfema\n",
    "        tmp_df.at[row.Index, 'ask_fast_ema'] = ((row.ask - tmp_df.at[row.Index -1, 'ask_fast_ema']) * fast_ema_factor) + prev_afema\n",
    "    \n",
    "    tmp_df = tmp_df[['datetime', 'bid_fast_ema', 'ask_fast_ema']]\n",
    "    new_df = new_df.merge(tmp_df, on='datetime', how='left')\n",
    "    \n",
    "    # Slow EMA\n",
    "    tmp_df = df.copy()\n",
    "    tmp_df.at[slow_ema_timestep -1, 'bid_slow_ema'] = tmp_df[['bid']][:slow_ema_timestep].values.mean()\n",
    "    tmp_df.at[slow_ema_timestep -1, 'ask_slow_ema'] = tmp_df[['ask']][:slow_ema_timestep].values.mean()\n",
    "    \n",
    "    tmp_df = tmp_df[slow_ema_timestep -1:].copy()\n",
    "    tmp_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    for row in tmp_df[1:].itertuples():\n",
    "        prev_bsema = tmp_df.at[row.Index -1, 'bid_slow_ema']\n",
    "        prev_asema = tmp_df.at[row.Index -1, 'ask_slow_ema']\n",
    "        \n",
    "        tmp_df.at[row.Index, 'bid_slow_ema'] = ((row.bid - tmp_df.at[row.Index -1, 'bid_slow_ema']) * slow_ema_factor) + prev_bsema\n",
    "        tmp_df.at[row.Index, 'ask_slow_ema'] = ((row.ask - tmp_df.at[row.Index -1, 'ask_slow_ema']) * slow_ema_factor) + prev_asema\n",
    "    \n",
    "    tmp_df = tmp_df[['datetime', 'bid_slow_ema', 'ask_slow_ema']]\n",
    "    new_df = new_df.merge(tmp_df, on='datetime', how='left')\n",
    "    \n",
    "    # EMA differences\n",
    "    new_df['bid_ema_diff'] = new_df['bid_fast_ema'] - new_df['bid_slow_ema']\n",
    "    new_df['ask_ema_diff'] = new_df['ask_fast_ema'] - new_df['ask_slow_ema']\n",
    "    \n",
    "    # Signal\n",
    "    tmp_df = new_df[new_df['bid_ema_diff'].isna() == False].copy()\n",
    "    tmp_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    tmp_df.at[signal_timestep -1, 'bid_macd_signal'] = tmp_df[['bid_ema_diff']][:signal_timestep].values.mean()\n",
    "    tmp_df.at[signal_timestep -1, 'ask_macd_signal'] = tmp_df[['ask_ema_diff']][:signal_timestep].values.mean()\n",
    "    \n",
    "    tmp_df = tmp_df[signal_timestep -1:].copy()\n",
    "    tmp_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    for row in tmp_df[1:].itertuples():\n",
    "        prev_bsignal = tmp_df.at[row.Index -1, 'bid_macd_signal']\n",
    "        prev_asignal = tmp_df.at[row.Index -1, 'ask_macd_signal']\n",
    "        \n",
    "        tmp_df.at[row.Index, 'bid_macd_signal'] = ((row.bid_ema_diff - tmp_df.at[row.Index -1, 'bid_macd_signal']) * signal_factor) + prev_bsignal\n",
    "        tmp_df.at[row.Index, 'ask_macd_signal'] = ((row.ask_ema_diff - tmp_df.at[row.Index -1, 'ask_macd_signal']) * signal_factor) + prev_asignal\n",
    "    \n",
    "    tmp_df = tmp_df[['datetime', 'bid_macd_signal', 'ask_macd_signal']]\n",
    "    new_df = new_df.merge(tmp_df, on='datetime', how='left')\n",
    "    \n",
    "    # Histogram\n",
    "    new_df['bid_macd_histogram'] = new_df['bid_ema_diff'] - new_df['bid_macd_signal']\n",
    "    new_df['ask_macd_histogram'] = new_df['ask_ema_diff'] - new_df['ask_macd_signal']\n",
    "    \n",
    "    return new_df.drop(columns=['bid_ema_diff', 'ask_ema_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation (Daily)\n",
    "day_df = aggregate(timeseries_df, rule='1D')\n",
    "vp.fast_stat(day_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation (Hourly)\n",
    "hour_df = aggregate(timeseries_df, rule='1H')\n",
    "vp.fast_stat(hour_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation (Minute)\n",
    "min_df = aggregate(timeseries_df, rule='1Min')\n",
    "vp.fast_stat(min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop N/A\n",
    "day_df.dropna(inplace=True)\n",
    "hour_df.dropna(inplace=True)\n",
    "min_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "EXEC_START = time.time()\n",
    "\n",
    "day_df.to_csv(f'{OUT_PATH_FILE}/DAT_ASCII_{currency_pair}_Day_{periods[0]}-{periods[-1]}.csv', index=False)\n",
    "hour_df.to_csv(f'{OUT_PATH_FILE}/DAT_ASCII_{currency_pair}_Hour_{periods[0]}-{periods[-1]}.csv', index=False)\n",
    "min_df.to_csv(f'{OUT_PATH_FILE}/DAT_ASCII_{currency_pair}_Min_{periods[0]}-{periods[-1]}.csv', index=False)\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation (Transformed Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_pair = 'EURUSD'\n",
    "filename      = f'DAT_ASCII_{currency_pair}_Day_201901-201906.csv'\n",
    "# filename      = f'DAT_ASCII_{currency_pair}_Hour_201901-201906.csv'\n",
    "# filename      = f'DAT_ASCII_{currency_pair}_Min_201901-201906.csv'\n",
    "\n",
    "source_file = f'{OUT_PATH_FILE}{filename}'\n",
    "df_chunks   = pd.read_csv(source_file, sep=',',\n",
    "                          parse_dates=['datetime'],\n",
    "                          date_parser=lambda x: pd.to_datetime(x, format='%Y-%m-%d %H:%M:%S'),\n",
    "                          chunksize=50_000)\n",
    "timeseries_df = pd.concat(df_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewed_handling(df, columns):\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    for column in columns:\n",
    "        new_df[f'{column}_log'] = np.log1p(df[column])\n",
    "        \n",
    "    return new_df\n",
    "\n",
    "def standard_scaler(df, columns):\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    scaled_values = StandardScaler().fit_transform(df[columns])\n",
    "    new_df = pd.concat([\n",
    "        new_df,\n",
    "        pd.DataFrame(scaled_values, columns=[f'{x}_norm' for x in columns])\n",
    "    ], axis=1)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform skewed data\n",
    "# No skewed data on oscillator values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "columns = [x for x in timeseries_df.columns if any([y for y in ['roc', 'rsi', 'stochastic', 'ema', 'macd'] if y in x])]\n",
    "timeseries_df = standard_scaler(timeseries_df, columns)\n",
    "timeseries_df.drop(columns=columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_evaluation(values):\n",
    "    shape = values.shape\n",
    "    print(shape)\n",
    "    \n",
    "    pca = PCA(n_components=shape[1])\n",
    "    pca.fit(values)\n",
    "    \n",
    "    # Reference: https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2896/pca-for-dimensionality-reduction-not-visualization/0/free-videos\n",
    "    # Evaluate by the variance, and try to preserve variance as high as 90%\n",
    "    expvar_percentages    = pca.explained_variance_ / np.sum(pca.explained_variance_)\n",
    "    cumexpvar_percentages = np.cumsum(expvar_percentages)\n",
    "    \n",
    "    return cumexpvar_percentages\n",
    "\n",
    "def pcaeval_plot(cumexpvar_percentages):\n",
    "    data = []\n",
    "    data.append(go.Scattergl(\n",
    "        x = [x for x in range(1, len(cumexpvar_percentages) +1)],\n",
    "        y = cumexpvar_percentages,\n",
    "        mode = 'lines+markers'\n",
    "    ))\n",
    "    \n",
    "    vp.plot_graph(data, 'PCA Evaluation',\n",
    "                  xlabel='N Features', ylabel='Cumulative Explained Variance',\n",
    "                  out_path=OUT_PATH_GRAPH)\n",
    "\n",
    "def pca_reduction(df, columns, n_component):\n",
    "    values = df[columns]\n",
    "    pca    = PCA(n_components=n_component)\n",
    "    reduced_values = pca.fit_transform(values)\n",
    "    \n",
    "    new_df = pd.concat([\n",
    "        df,\n",
    "        pd.DataFrame(reduced_values, columns=[f'pca_{x}' for x in range(1, n_component +1)])\n",
    "    ], axis=1)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA evaluation\n",
    "columns = [x for x in timeseries_df.columns if any([y for y in ['roc', 'rsi', 'stochastic', 'ema', 'macd'] if y in x])]\n",
    "values  = timeseries_df[columns].values\n",
    "cumexpvar_percentages = pca_evaluation(values)\n",
    "\n",
    "pcaeval_plot(cumexpvar_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA reduction\n",
    "n_component   = 4\n",
    "columns       = [x for x in timeseries_df.columns if any([y for y in ['roc', 'rsi', 'stochastic', 'ema', 'macd'] if y in x])]\n",
    "timeseries_df = pca_reduction(timeseries_df, columns, n_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scaler(df, columns):\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    scaled_values = MinMaxScaler(feature_range=(0, 1)).fit_transform(df[columns])\n",
    "    new_df = pd.concat([\n",
    "        new_df,\n",
    "        pd.DataFrame(scaled_values, columns=[f'{x}_norm' for x in columns])\n",
    "    ], axis=1)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns       = [x for x in timeseries_df.columns if 'pca_' in x]\n",
    "timeseries_df = minmax_scaler(timeseries_df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "EXEC_START = time.time()\n",
    "\n",
    "# Decimal rounding\n",
    "for column in [x for x in timeseries_df.columns if x != 'datetime']:\n",
    "    timeseries_df[column] = np.round(timeseries_df[column], 5)\n",
    "timeseries_df.to_csv(f'{OUT_PATH_FILE}/DAT_ASCII_{currency_pair}_Normalized.csv', index=False)\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
